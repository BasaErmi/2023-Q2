## OCR系统建模报告

### 类图

#### paddleOCR

```mermaid
classDiagram
    direction TB

    class PPOCRModel {
        -index_to_word: Vec<String>
        -model: ModelType
        -inference_count: RefCell<usize>
        -inference_time: RefCell<Duration>
        +new_from_file(onnx_file: P1, words_file: P2) Result<PPOCRModel>
        +new(onnx: &[u8], index_to_word: Vec<String>) Result<PPOCRModel>
        +get_average_inference_time() Option<Duration>
        +image_to_text(image: &RgbImage, _is_preprocessed: bool) Result<String>
    }

    class PPOCRChV4RecInfer {
        -model: PPOCRModel
        +new() Result<Self>
        +image_to_text(image: &RgbImage, is_preprocessed: bool) Result<String>
    }

    class ImageToText {
        <<interface>>
        +image_to_text(image: &RgbImage, is_preprocessed: bool) Result<String>
    }

    class preprocess {
        +resize_img(rec_image_shape: Shape3D, img: &RgbImage) RgbImage
        +normalize_image_to_ndarray(img: &RgbImage) ndarray::Array4<f32>
        +normalize_image_to_tensor(img: &RgbImage) Tensor
    }

    PPOCRModel --|> ImageToText
    PPOCRChV4RecInfer --> PPOCRModel
    preprocess <.. PPOCRModel : uses
    preprocess <.. PPOCRChV4RecInfer : uses

```



#### YasOCR

```mermaid
classDiagram
    direction LR
    class User {
        +uploadImage()
        +getRecognizedText()
    }

    class YasOCRModel {
        +new(model: &[u8], content: &str) Result<YasOCRModel>
        +image_to_text(image: &RgbImage, is_preprocessed: bool) Result<String>
        +image_to_text(image: &ImageBuffer<Luma<f32>, Vec<f32>>, is_preprocessed: bool) Result<String>
        +image_to_text(im: &GrayImage, is_preprocessed: bool) Result<String>
        +get_average_inference_time() f64
    }

    class preprocess {
        +to_gray(raw: &RgbImage) ImageBuffer<Luma<f32>, Vec<f32>>
        +pre_process(im: ImageBuffer<Luma<f32>, Vec<f32>>) (ImageBuffer<Luma<f32>, Vec<f32>>, bool)
    }

    class image {
        +ImageBuffer
        +Luma
        +RgbImage
        +GenericImageView
        +imageops
    }

    User --> YasOCRModel : Uses
    YasOCRModel --> preprocess : Uses
    YasOCRModel --> image : Uses
    preprocess --> image : Uses
```



### 状态图

#### paddleOCR

```mermaid
stateDiagram-v2
    [*] --> Initializing
    Initializing --> LoadingModel: Initialize PPOCRModel
    LoadingModel --> ModelLoaded: Model loaded successfully
    LoadingModel --> Error: Error loading model
    ModelLoaded --> WaitingForImage: Ready for image input
    WaitingForImage --> ResizingImage: Image received
    ResizingImage --> NormalizingImage: Image resized
    NormalizingImage --> RunningInference: Image normalized
    RunningInference --> ParsingOutput: Inference run
    ParsingOutput --> GeneratingText: Output parsed
    GeneratingText --> TextGenerated: Text generated
    TextGenerated --> WaitingForImage: Ready for next image
    Error --> [*]: Error handled

```

#### YasOCR

```mermaid
stateDiagram-v2
    state "Start" as S0
    state "Input RGB Image" as S1
    state "Convert to Gray Image" as S2
    state "Normalize Image" as S3
    state "Check if Monochrome" as S4
    state "Crop Image" as S5
    state "Normalize Again" as S6
    state "Resize and Pad Image" as S7
    state "Binarize Image" as S8
    state "OCR Inference" as S9
    state "Output Text" as S10
    state "End" as S11

    S0 --> S1 : Receive RGB Image
    S1 --> S2 : Call to_gray()
    S2 --> S3 : Call pre_process() - Normalize Step
    S3 --> S4
    S4 --> S5 : Non-monochrome
    S4 --> S11 : Monochrome
    S5 --> S6 : Call crop()
    S6 --> S7 : Normalize Again
    S7 --> S8 : Call resize_and_pad()
    S8 --> S9 : Binarize and finalize preprocessing
    S9 --> S10 : Call inference_string()
    S10 --> S11 : Output OCR Result
    S11 --> S0 : Ready for Next Image

```

### 活动图

#### paddleOCR



```mermaid
graph TD
    A[Start] --> B[Initialize PPOCRModel]
    B --> C{Load Model}
    C -->|Success| D[Model Loaded]
    C -->|Failure| E[Error Loading Model]
    D --> F[Wait for Image Input]
    F --> G[Receive Image]
    G --> H[Resize Image]
    H --> I[Normalize Image]
    I --> J[Run Inference]
    J --> K[Parse Output]
    K --> L[Generate Text]
    L --> M[Return Text]
    M --> F
    E --> N[Handle Error]
    N --> O[End]

    style A fill:#f9f,stroke:#333,stroke-width:4px
    style O fill:#f9f,stroke:#333,stroke-width:4px

```



#### YasOCR

```mermaid
graph TD
    A(Start) --> B[Receive RGB Image]
    B --> C[Convert to Gray Image]
    C --> D[Normalize Image]
    D --> E{Is Monochrome?}
    E -- Yes --> F[Return Empty String]
    E -- No --> G[Crop Image]
    G --> H[Normalize Again]
    H --> I[Resize and Pad Image]
    I --> J[Binarize Image]
    J --> K[Perform OCR Inference]
    K --> L[Output Recognized Text]
    L --> A

    classDef startEnd fill:#f96,stroke:#333,stroke-width:2px;
    class A,F,L startEnd;

```



### 用例描述

#### paddleOCR

##### 用例 1: 创建并初始化 OCR 模型

概述：用户希望创建并初始化一个 OCR 模型实例，从文件或内存中加载模型和字符索引映射文件。

**参与者**：用户

**前置条件：**用户具有 ONNX 模型文件和字符索引映射文件，或者具有嵌入的模型和字符索引映射字节数组。

**基本事件流：**

1. 用户调用 `PPOCRModel::new_from_file` 方法，传入 ONNX 模型文件路径和字符索引映射文件路径。
   - 系统读取字符索引映射文件并解析内容。
   - 系统根据配置特性 (`ort` 或 `tract_onnx`) 加载 ONNX 模型文件。
   - 系统创建并返回一个 `PPOCRModel` 实例。
2. 用户调用 `PPOCRModel::new` 方法，传入嵌入的模型字节数组和字符索引映射。
   - 系统解析字符索引映射。
   - 系统根据配置特性 (`ort` 或 `tract_onnx`) 加载模型字节数组。
   - 系统创建并返回一个 `PPOCRModel` 实例。

**后置条件**：系统创建了一个 `PPOCRModel` 实例，可以用于 OCR 推理。

##### 用例 2: 从图像中提取文本

**概述：**用户希望使用 OCR 模型从图像中提取文本。

**参与者**：用户

**前置条件**：

- 用户已经创建并初始化了一个 `PPOCRModel` 或 `PPOCRChV4RecInfer` 实例。
- 用户具有待识别的图像。

**主要事件流程：**

1. 用户调用 `PPOCRChV4RecInfer::image_to_text` 方法，传入待识别的图像。
   - 系统记录开始时间。
   - 系统调用 `resize_img` 函数调整图像大小。
   - 系统根据配置特性 (`ort` 或 `tract_onnx`) 归一化图像。
   - 系统运行模型推理。
   - 系统解析模型输出，生成文本结果。
   - 系统记录推理时间并更新推理统计数据。
   - 系统返回识别结果文本。

**后置条件：**

- 系统返回了识别出的文本结果。
- 系统更新了模型的推理统计数据。

##### 用例 3: 创建 OCR 推理实例

**概述**：用户希望创建一个 OCR 推理实例 `PPOCRChV4RecInfer`，以便进行中文文本的识别。

**参与者**：用户

**前置条件：**用户具有用于推理的 ONNX 模型文件和字符索引映射文件。

**主要事件流程：**

1. 用户调用 `PPOCRChV4RecInfer::new` 方法。
   - 系统调用 `ppocr_model!` 宏创建 `PPOCRModel` 实例。
   - 系统返回 `PPOCRChV4RecInfer` 实例。

**后置条件：**系统创建了一个 `PPOCRChV4RecInfer` 实例，可以用于中文文本的 OCR 推理。



#### YasOCR

##### 用例1：上传图像

**参与者**：用户

**前置条件**：用户已经准备好要识别的图像文件。

**触发**：用户启动OCR应用程序，并选择上传图像。

**基本事件流**：
1. 用户点击“上传图像”按钮。
2. 系统显示文件选择对话框。
3. 用户选择要上传的图像文件。
4. 系统将图像文件上传并传递给`YasOCRModel`进行处理。

**后置条件**：图像文件成功上传，并准备进行预处理和识别。

**扩展点**：文件选择过程中可能发生的错误（如取消选择文件）。

##### 用例2：获取识别文本

**参与者**：用户

**前置条件**：用户已经上传了图像文件。

**触发**：用户点击“开始识别”按钮。

**基本事件流：**

1. 用户点击“开始识别”按钮。
2. 系统调用`YasOCRModel`的`image_to_text`方法，传递上传的图像。
3. `YasOCRModel`使用`preprocess`模块对图像进行预处理，包括灰度化、归一化、裁剪、调整尺寸等。
4. `YasOCRModel`调用OCR模型进行推理，获取文本结果。
5. 系统将识别结果显示给用户。

**后置条件**：系统显示识别出的文本结果。

**扩展点**：处理过程中可能发生的错误（如图像预处理失败、模型推理失败）。





架构设计：

#### 类和方法对应的用例

#### `YasOCRModel::new`

- **用例**：初始化OCR模型
- **描述**：加载模型文件和字符映射表。

#### `YasOCRModel::image_to_text`

- **用例**：图像到文本的转换
- **描述**：接收输入图像（RGB图像、灰度图像等），进行预处理，然后调用模型进行推理，返回识别出的文本。

#### `preprocess::to_gray`

- **用例**：图像预处理 - 灰度化
- **描述**：将RGB图像转换为浮点型灰度图像。

#### `preprocess::pre_process`

- **用例**：图像预处理 - 综合处理
- **描述**：对灰度图像进行归一化、裁剪、调整尺寸等预处理操作。

### 主要场景示例

1. **用户上传图像并识别文本**
    - 用户启动应用，上传一张RGB图像。
    - 系统调用`YasOCRModel::image_to_text`进行识别。
    - `YasOCRModel`调用`preprocess::to_gray`将图像转换为灰度图。
    - `YasOCRModel`调用`preprocess::pre_process`对灰度图进行处理。
    - 系统调用OCR模型进行推理，返回识别结果。
    - 识别结果显示在用户界面上。
