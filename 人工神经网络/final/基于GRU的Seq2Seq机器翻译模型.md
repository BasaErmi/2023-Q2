# åŸºäºä¸¤å±‚GRUæ¶æ„çš„Seq2Seqä¸­è¯‘è‹±æœºå™¨ç¿»è¯‘æ¨¡å‹

|   å­¦å·   |  å§“å  |
| :------: | :----: |
| 20319045 | åˆ˜å† éºŸ |

## æ‘˜è¦

æœ¬æ­¤å®éªŒå‚è€ƒäº†Pytorchå®˜æ–¹æ–‡æ¡£ä¸­çš„æœºå™¨ç¿»è¯‘æ•™ç¨‹ï¼Œå®ç°äº†åŸºäºRNNæ¶æ„çš„Seq2Seqä¸­è¯‘è‹±æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œå…¶ä¸­ç¼–ç å™¨Encoderå’Œè§£ç å™¨Decoderåˆ†åˆ«é‡‡ç”¨äº†**ä¸¤å±‚GRU**æ¶æ„å®ç°ï¼Œå¹¶ä¸”å®ç°äº†**æ³¨æ„åŠ›æœºåˆ¶**ï¼Œä½¿æ¨¡å‹å¯ä»¥åŠ¨æ€åœ°é€‰æ‹©ç¼–ç å™¨è¾“å‡ºçš„ä¸åŒéƒ¨åˆ†ä»¥æå‡ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ä½¿ç”¨**è¯å‘é‡åˆå§‹åŒ–**å’Œ**Teacher Foringç­–ç•¥**ï¼Œä»¤æ¨¡å‹æ‹¥æœ‰æ›´å¿«ã€æ›´å¥½çš„æ”¶æ•›æ•ˆæœã€‚

å®éªŒä¸­é€šè¿‡ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼Œå……åˆ†å¯¹æ¯”è®¨è®ºäº†RNNå±‚æ·±åº¦ã€Teachingç­–ç•¥ã€é¢„è®­ç»ƒè¯å‘é‡ä»¥åŠæ•°æ®é›†å¤§å°å¯¹æ¨¡å‹æ”¶æ•›èƒ½åŠ›å’Œæ•ˆæœçš„å½±å“ï¼Œå¹¶åˆ†åˆ«åœ¨10kå’Œ100kæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤šæ¬¡å¯¹æ¯”è¯•éªŒã€‚

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾—ç›Šäºattentionæœºåˆ¶ã€ä½¿ç”¨æ›´å¤§è§„æ¨¡çš„æ•°æ®é›†ä»¥åŠé¢„è®­ç»ƒè¯å‘é‡ï¼Œå®ç°çš„æ¨¡å‹é€šè¿‡**100kæ•°æ®é›†**è®­ç»ƒä¹‹åèƒ½åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°**0.03697(3.7%)**çš„BLEUåˆ†æ•°ï¼Œé«˜äºå®éªŒç»™å‡ºçš„0.003çš„baselineã€‚

## å®éªŒä»»åŠ¡

### ä»»åŠ¡æè¿°

åœ¨ç»™å®šä¸­è‹±æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ„å»ºåŸºäºGRUæˆ–è€…LSTMçš„Seq2Seqæ¶æ„ï¼ˆå…¶ä¸­ç¼–ç å™¨å’Œè§£ç å™¨å‡æ·±åº¦ä¸º2ã€ä¸”ä¸ºå•å‘ï¼‰ä¸­è¯‘è‹±æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œå¹¶ä¸”å®ç°attentionæœºåˆ¶ã€‚

### æ•°æ®é›†æè¿°

è®­ç»ƒé›†å…±æœ‰ 4 ä¸ª jsonl æ–‡ä»¶ï¼Œåˆ†åˆ«å¯¹åº”ç€è®­ç»ƒé›†ï¼ˆå°ï¼‰ã€è®­ç»ƒé›†ï¼ˆå¤§ï¼‰ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œå®ƒä»¬çš„å¤§å°åˆ†åˆ«æ˜¯ 100kã€10kã€500ã€200ã€‚jsonl æ–‡ä»¶ä¸­çš„æ¯ä¸€è¡ŒåŒ…å«ä¸€ä¸ªå¹³è¡Œè¯­æ–™æ ·æœ¬ã€‚æ¨¡å‹çš„æ€§èƒ½ä»¥æµ‹è¯•é›†çš„ç»“æœä¸ºæœ€ç»ˆæ ‡å‡†ã€‚

| æ•°æ®é›†åç§° | æ ·æœ¬è§„æ¨¡ |
| :--------: | :------: |
|    test    |   200    |
|   vaild    |   500    |
| train_10k  |   10k    |
| train_100k |   100k   |

## æ•°æ®é¢„å¤„ç†

å¯¹äºå¦‚ä¸‹å½¢å¼çš„jsonlæ ¼å¼åŸå§‹æ•°æ®é›†ï¼š

```json
{"en": "\"Sounds like you are locked,\" the Deputy Commandant replied.", "zh": "â€œå¬èµ·æ¥ä½ è¢«é”ä½äº†å•Šï¼Œâ€å‰¯å¸ä»¤å›å¤é“ã€‚", "index": 2}
```

éœ€è¦å°†å…¶è½¬æ¢æˆå¦‚ä¸‹è¯­å¥å¯¹å½¢å¼çš„txtæ–‡æœ¬ï¼š

```
sounds like you are locked the deputy commandant replied@@å¬ èµ·æ¥ ä½  è¢« é”ä½ äº† å•Š å‰¯å¸ä»¤ å›å¤ é“
```

### æ ·æœ¬è¯»å–

é¦–å…ˆè¯»å–jsonlæ–‡ä»¶ï¼š

```python
# è¯»å–jsonlæ–‡ä»¶
def read_jsonl(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            data.append(json.loads(line.strip()))
    return data
```

ç„¶åæ ¹æ®jsonæ–‡ä»¶çš„å‰ç¼€åˆ¤æ–­æ–‡æœ¬å±äºå“ªä¸€ç§è¯­è¨€å¹¶æå–ï¼š

```python
# æå–éœ€è¦çš„å­—æ®µ
def extract_fields(data):
    en_texts = [item['en'] for item in data]
    zh_texts = [item['zh'] for item in data]
    return en_texts, zh_texts
```

### ç¬¦å·å»é™¤ä¸æ ¼å¼æ ‡å‡†åŒ–

æå–å‡ºå¯¹åº”çš„`en_texts`å’Œ`zh_texts`æ–‡æœ¬åï¼Œé€šè¿‡å¼•å…¥æ­£åˆ™è¡¨è¾¾å¼å»é™¤æ–‡æœ¬ä¸­å¯èƒ½å­˜åœ¨çš„ç¬¦å·ï¼ˆè®¸å¤šè¿˜æ˜¯æ•°æ®é›†åŸæœ‰çš„å¼‚å¸¸ç¬¦å·ï¼‰ï¼š

```python
    if lang == 'en':
        # åˆ é™¤è‹±æ–‡æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[.,?!â€“\-":;()"â€™â€œâ€\'Â·/â€˜â€™\[\]â€¦`Â¾]', '', text)
    else:
        # åˆ é™¤ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[ã€‚ï¼Ÿï¼Œï¼â€“\-â€œâ€ï¼šï¼›ï¼ˆ()ï¼‰ã€â€”ã€Šã€‹<>Â·,:;?!./â€˜â€™â€¦\[\]"Â¾]', '', text)
        # åœ¨ç™¾åˆ†å·å‰åŠ ç©ºæ ¼
        text = re.sub(r'%', ' %', text)
```

å¯¹äºä¸­æ–‡æ–‡æœ¬ï¼Œæœ‰çš„æ ·æœ¬ä¸­å¯¹äºäººåã€åœ°åç­‰ç‰¹æ®Šåç§°ï¼Œä¼šå­˜åœ¨ç”¨æ‹¬å·å†…åŠ ä¸Šè‹±æ–‡æ³¨é‡Šçš„æƒ…å†µã€‚å¯¹äºè¿™ç§æƒ…å†µä¸ºäº†ä¸å½±å“è®­ç»ƒæ•ˆæœï¼Œéœ€è¦å°†æ•´ä¸ªæ‹¬å·é™¤å»ï¼Œå³ï¼š

```
ä½œè€…æ˜¯ä¿å®ˆæ´¾è‹±å›½è®°è€…é©¬ä¸Â·æ²ƒå°”å¤«ï¼ˆMartin Wolf ï¼‰
->
ä½œè€…æ˜¯ä¿å®ˆæ´¾è‹±å›½è®°è€…é©¬ä¸æ²ƒå°”å¤«
```

ä»£ç å¦‚ä¸‹ï¼š

```python
        # å¯¹äºæ‹¬å·åŠ è‹±æ–‡çš„å½¢å¼è¿›è¡Œåˆ é™¤
        text = re.sub(r'\([A-Za-z\s]+\)', '', text)
```

åŒæ—¶ï¼Œæ•°æ®é›†ä¸­è¿˜æœ‰è®¸å¤šæ•°å­—ï¼Œæ¯”å¦‚`1999`ã€Â·`2008`ç­‰ï¼Œå¯¹äºè¿™ç±»ç‰¹æ®Šæ•°å­—ä¸èƒ½ç®€å•å½“ä½œä¸€ä¸ªè¯æ±‡ï¼ˆä¸å…·æœ‰æ³›åŒ–æ€§ï¼‰ï¼Œè€Œæ˜¯åº”è¯¥æ‹†åˆ†æˆæ•°å­—`1`ã€`9`ã€`9`ã€`9`ç­‰ï¼Œå³æ‰€æœ‰çš„æ•°å­—ç»„åˆå¯ä»¥é€šè¿‡10ä¸ªå•ä¸ªæ•°å­—çš„è¯æ±‡è¿›è¡Œè¡¨ç¤ºã€‚è¿™æ ·ä¸ä»…èƒ½å¤Ÿå¤§å¹…å‡å°‘è¯æ±‡é‡ï¼Œè€Œä¸”èƒ½å¤Ÿæå‡æ¨¡å‹å¯¹äºæ•°å­—çš„å¤„ç†èƒ½åŠ›ã€‚

ç”±äºä½¿ç”¨çš„åˆ†è¯æ¨¡å‹ä¸ä¼šå°†æ•°å­—åˆ†å¼€ï¼Œæ‰€ä»¥è¿™é‡Œè¿˜éœ€è¦å¯¹æ•°å­—è¿›è¡Œå¤„ç†ï¼Œå³åœ¨æ•°å­—ä¹‹é—´åŠ å…¥ç©ºæ ¼ï¼Œè¿™æ ·åˆ†è¯æ¨¡å‹å°±ä¼šå°†å…¶è¯†åˆ«æˆå¤šä¸ªå­—ç¬¦ï¼š

```python
    # æ‹†åˆ†è¿ç»­æ•°å­—
    text = re.sub(r'(\d)', r' \1 ', text)
```

### åˆ†è¯

ç„¶åä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†è¯æ¨¡å‹å°†è¾“å…¥å¥å­åˆ‡åˆ†ä¸º tokensï¼Œæ¯ä¸ªå­ä¸²ç›¸å¯¹æœ‰ç€å®Œæ•´çš„è¯­ä¹‰ï¼Œä¾¿äºå­¦ä¹  embedding è¡¨è¾¾ã€‚

åœ¨æœ¬æ¬¡å®éªŒä¸­å¯¹äºè‹±æ–‡æ–‡æœ¬ï¼Œä½¿ç”¨`NLTK`åº“ä¸­çš„`word_tokenize`åˆ†è¯æ¨¡å‹è¿›è¡Œåˆ†è¯ï¼›å¯¹äºä¸­æ–‡æ–‡æœ¬åˆ™ä½¿ç”¨`jieba`åº“è¿›è¡Œåˆ†è¯ï¼š

```python
# åˆ†è¯å‡½æ•°
def tokenize(text, lang='en'):
    if lang == 'en':
        tokens = word_tokenize(text)
    else:
        tokens = jieba.lcut(text)
    result = ' '.join(tokens)
    result = re.sub(r'\s+', ' ', result)  # å°†å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ªç©ºæ ¼
    return result
```

### å¤„ç†å¹¶ä¿å­˜

å°†è¯»å–åçš„æ–‡æœ¬ä¸­è‹±æ–‡å•è¯å…¨éƒ¨è½¬åŒ–ä¸ºå°å†™ï¼ˆä¸ºäº†è¡¨è¾¾ç»Ÿä¸€ï¼ŒåŒæ—¶é™ä½è¯æ±‡è¡¨å¤§å°ï¼‰åç»è¿‡ä¸Šè¿°å‡½æ•°è¿›è¡Œæ ‡ç‚¹å¤„ç†å’Œåˆ†è¯ï¼Œç„¶åå°†å¥å­å¯¹æ‹¼æ¥åœ¨ä¸€è¡Œä¸­ï¼Œå¹¶ä½¿ç”¨`@@`æ ‡è¯†ç¬¦è¡¨ç¤ºåˆ†å‰²ï¼Œå†™å…¥txtæ–‡æœ¬ä¸­ã€‚

```python
# ä¿å­˜å¤„ç†åçš„æ•°æ®ä¸ºtxtæ–‡ä»¶ï¼Œä½¿ç”¨@@åˆ†å‰²å¥å­å¯¹
def save_to_txt(en_texts, zh_texts, output_file):
    with open(output_file, 'w', encoding='utf-8') as file:
        for en_text, zh_text in zip(en_texts, zh_texts):
            # è½¬åŒ–ä¸ºå°å†™
            en_text = en_text.lower()
            zh_text = zh_text.lower()
            # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œæ‹†åˆ†è¿ç»­æ•°å­—
            en_text = remove_punctuation_and_split_digits(en_text, lang='en')
            zh_text = remove_punctuation_and_split_digits(zh_text, lang='zh')
            # åˆ†è¯
            en_text = tokenize(en_text, lang='en')
            zh_text = tokenize(zh_text, lang='zh')
            file.write(f'{en_text}@@{zh_text}\n')
```

### æ„å»ºè¯å…¸

#### langç±»

åˆ›å»º`lang`ç±»ï¼Œæ„å»ºä»å•è¯åˆ°ç´¢å¼•å’Œä»ç´¢å¼•åˆ°å•è¯çš„æ˜ å°„ã€‚åŒæ—¶åœ¨`lang`ç±»ä¸­å®šä¹‰ä¸‰ä¸ªç‰¹æ®Šæ ‡è¯†ç¬¦ä¸ºå‰ä¸‰ä¸ªç´¢å¼•ï¼Œç”¨æ¥å¤„ç†è¯å…¸çš„ç‰¹æ®Šæƒ…å†µï¼š

- `<SOS>:0`ï¼šè¡¨ç¤ºå¥å­å¼€å§‹
- `<EOS>:1`ï¼šè¡¨ç¤ºå¥å­ç»“æŸ
- `<UNK>:2`ï¼šè¡¨ç¤ºæœªçŸ¥ï¼Œå½“å¤„ç†çš„è¯æ±‡ä¸åœ¨è¯æ±‡è¡¨ä¸­åˆ™å°†å…¶æ ‡è¯†ä¸º`<UNK>`ã€‚

langç±»ä»£ç å¦‚ä¸‹ï¼Œç±»ä¸­åˆ›å»ºä»å•è¯åˆ°ç´¢å¼•çš„åŒå‘æ˜ å°„ï¼Œå¹¶ä¸”é€šè¿‡åˆ†è¯åçš„ç©ºæ ¼åˆ’åˆ†å•è¯ï¼Œç„¶åæ„å»ºå½“å‰æ•°æ®é›†çš„è¯æ±‡è¡¨ï¼š

```python
class Lang:
    def __init__(self, name):
        self.name = name  # è¯­è¨€çš„åç§°
        self.word2index = {}  # å•è¯åˆ°ç´¢å¼•çš„æ˜ å°„
        self.word2count = {}  # å•è¯å‡ºç°æ¬¡æ•°çš„ç»Ÿè®¡
        self.index2word = {0: "<SOS>", 1: "<EOS>", 2:"<UNK>"}  # ç´¢å¼•åˆ°å•è¯çš„æ˜ å°„ï¼Œåˆå§‹åŒ…å«SOSå’ŒEOSä»¥åŠUNK
        self.n_words = 3  # å•è¯æ•°ç›®åˆå§‹åŒ–ä¸º3

    def addSentence(self, sentence):
        # å°†å¥å­æ‹†åˆ†æˆå•è¯ï¼Œå¹¶é€ä¸ªæ·»åŠ åˆ°å­—å…¸ä¸­
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        # å¦‚æœå•è¯ä¸åœ¨å­—å…¸ä¸­ï¼Œæ·»åŠ è¯¥å•è¯
        if word not in self.word2index:
            self.word2index[word] = self.n_words  # ç»™å•è¯åˆ†é…ä¸€ä¸ªæ–°çš„ç´¢å¼•
            self.word2count[word] = 1  # åˆå§‹åŒ–å•è¯çš„è®¡æ•°ä¸º1
            self.index2word[self.n_words] = word  # å°†æ–°ç´¢å¼•å’Œå•è¯æ·»åŠ åˆ°ç´¢å¼•åˆ°å•è¯çš„æ˜ å°„ä¸­
            self.n_words += 1  # å¢åŠ å•è¯æ•°ç›®
        else:
            self.word2count[word] += 1  # å¦‚æœå•è¯å·²å­˜åœ¨ï¼Œå¢åŠ è¯¥å•è¯çš„è®¡æ•°
```

#### æ„å»ºè¯­è¨€å¯¹è±¡

è¯»å–æŒ‡å®šè¯­è¨€å¯¹çš„æ–‡æœ¬æ–‡ä»¶ï¼Œé€šè¿‡ä¹‹å‰å®šä¹‰çš„`@@`æ ‡è¯†ç¬¦åˆ†å‰²ä¸åŒç±»å‹çš„è¯­å¥ï¼Œç„¶ååˆ›å»ºç›¸åº”çš„è¯­è¨€å¯¹è±¡ï¼Œå¹¶è¿”å›è¿™äº›å¯¹è±¡å’Œå¥å­å¯¹ï¼š

```python
def readLangs(lang1, lang2, reverse=False, text_type='train'):
    print("Reading lines...")
    

    # è¯»å–æ–‡ä»¶å¹¶æŒ‰è¡Œåˆ†å‰²
    lines = open('data/sentence_pairs/%s-%s-%s.txt' % (lang1, lang2, text_type), encoding='utf-8').\
        read().strip().split('\n')

    # å°†æ¯ä¸€è¡Œåˆ†å‰²æˆå¯¹å¹¶è¿›è¡Œè§„èŒƒåŒ–
    pairs = [[s for s in l.split('@@')] for l in lines] #å°†æ¯ä¸€è¡ŒæŒ‰ç…§åˆ¶è¡¨ç¬¦@@åˆ†å‰²æˆå¥å­å¯¹,å¹¶ä¸”æ¯ä¸ªå¥å­å¯¹éƒ½é€šè¿‡æ ‡å‡†åŒ–å¤„ç†
```

#### è¿‡æ»¤è¶…å‡ºé•¿åº¦çš„å¥å­

å¯¹`train_10k`æ•°æ®é›†è¿›è¡Œå¥å­é•¿åº¦ç»Ÿè®¡åˆ†æï¼Œå¯ä»¥å¾—åˆ°å…¶é•¿åº¦çš„æ•°é‡ç›´æ–¹å›¾ï¼š

<img src="./assets/image-20240708182152721.png" alt="image-20240708182152721" style="zoom:50%;" />

å¥å­éšé•¿åº¦å¤§è‡´å‘ˆæ­£æ€åˆ†å¸ƒï¼Œç»å¤§å¤šæ•°å¥å­éƒ½åœ¨é•¿åº¦60ä»¥å†…ï¼Œæ‰€ä»¥è¿™é‡Œé€‰æ‹©è¿‡æ»¤æ‰é•¿åº¦è¶…è¿‡50çš„å¥å­ï¼š

```python
"""è¿‡æ»¤å¥å­å¯¹"""

# å®šä¹‰æœ€å¤§å¥å­é•¿åº¦
MAX_LENGTH = 60

def filterPair(p):
    # æ£€æŸ¥å¥å­å¯¹æ˜¯å¦ç¬¦åˆé•¿åº¦é™åˆ¶
    return len(p[0].split(' ')) < MAX_LENGTH and \
        len(p[1].split(' ')) < MAX_LENGTH

def filterPairs(pairs):
    # å¯¹æ‰€æœ‰å¥å­å¯¹è¿›è¡Œè¿‡æ»¤ï¼Œåªä¿ç•™ç¬¦åˆæ¡ä»¶çš„å¥å­å¯¹
    return [pair for pair in pairs if filterPair(pair)]
```

å‡†å¤‡ç©æ•°æ®é›†åè¿›è¡Œè¯»å–`train_100k`æ•°æ®é›†ï¼Œå¯ä»¥å¾—åˆ°å¦‚ä¸‹è¾“å‡ºï¼š

```
Reading lines...
Read 100000 sentence pairs
Trimmed to 97849 sentence pairs
Counting words...
Counted words:
zh 53090
eng 43383
```

å³è¿‡æ»¤æ‰äº†é•¿åº¦è¶…è¿‡60çš„å¥å­åè¿˜æœ‰97849æ¡å¥å­ï¼›ä¸æ­¤åŒæ—¶æ•°æ®é›†ç»„æˆçš„è¯æ±‡è¡¨ä¸­å…±æœ‰ä¸­æ–‡53090ä¸ªè¯ã€ä»¥åŠè‹±æ–‡43383ä¸ªè¯ã€‚

#### è¿‡æ»¤è¯é¢‘è¿‡å°çš„è¯

ä¸ºäº†ç¼©å‡è¯æ±‡è¡¨å¤§å°ï¼Œé¿å…è¯æ±‡é‡è¿‡å¤§ï¼Œè¿™é‡Œè¿˜å°†å¯¹è¯é¢‘å°äº3çš„è¯è¿›è¡Œè¿‡æ»¤ï¼Œç”±æ­¤å‡å°‘æ•°æ®çš„ç»´åº¦æå‡è®¡ç®—æ•ˆç‡ã€‚

åŒæ—¶é¢‘ç‡è¿‡ä½çš„è¯å¾€å¾€æ˜¯å™ªéŸ³ï¼Œå¯èƒ½æ˜¯æ‹¼å†™é”™è¯¯ã€ä¸“æœ‰åè¯æˆ–è€…ä¸å¸¸ç”¨çš„è¯è¯­ï¼Œè¿‡æ»¤æ‰è¿™äº›è¯å¯ä»¥æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆæœï¼›è¿˜æœ‰åŠ©äºæ¨¡å‹æ›´ä¸“æ³¨äºå¸¸è§è¯ï¼Œæé«˜å¯¹æµ‹è¯•æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚

åˆ©ç”¨ä¹‹å‰`lang`ç±»ä¸­è®°å½•çš„è¯é¢‘ï¼Œå¯¹å¯¹è¯é¢‘å°äº5çš„è¯è¿›è¡Œè¿‡æ»¤ï¼š

```python
# è¿‡æ»¤æ‰è¯é¢‘å°äº3çš„è¯
MIN_COUNT = 3

def trimRareWords(lang, pairs):
    print('trimming rare words...')
    # ä»…ä¿ç•™è¯é¢‘å¤§äºç­‰äºMIN_COUNTçš„å•è¯
    lang.word2index = {word: idx for word, idx in lang.word2index.items() if lang.word2count[word] >= MIN_COUNT}
    lang.index2word = {idx: word for word, idx in lang.word2index.items()}
    lang.n_words = len(lang.word2index)
    
    print('keep words:', lang.n_words)
    
    return lang

# è¿‡æ»¤æ‰è¯é¢‘å°äº3çš„è¯
input_lang1 = trimRareWords(input_lang, pairs)
output_lang1 = trimRareWords(output_lang, pairs)
```

å¯ä»¥å¾—åˆ°å¤„ç†åçš„ä¸­è‹±æ–‡çš„è¯æ±‡è¡¨éƒ½ç¼©å‡äº†`50%`å·¦å³çš„æ•°æ®ã€‚å¯ä»¥çœ‹åˆ°æ•°æ®é›†ä¸­èµ·ç å¤§çº¦ä¸€åŠå·¦å³éƒ½æ˜¯éå¸¸ç”¨è¯ï¼Œå°†è¿™äº›è¯æ»¤é™¤åå¯ä»¥è¾ƒå¤§æå‡æ•°æ®é›†è´¨é‡ï¼Œè€Œä¸”ç¼©å‡äº†è¿‘ä¸€èˆ¬çš„ç»´åº¦ï¼Œå¯ä»¥æœ‰æ•ˆæå‡è®­ç»ƒé€Ÿåº¦ã€‚

```
trimming rare words...
keep words: 23910
trimming rare words...
keep words: 21674
```

### é¢„è®­ç»ƒè¯å‘é‡åˆå§‹åŒ–

ç”±äºå®éªŒç»™å‡ºçš„å¤§æ•°æ®é›†ï¼ˆ100kï¼‰å¯¹äºNLPä»»åŠ¡æ¥è¯´ä¾æ—§ç®—æ˜¯å°æ•°æ®é›†ï¼Œå¦‚æœä»é›¶å¼€å§‹è®­ç»ƒè¯å‘é‡å¾ˆå®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œè€Œä¸”æ— æ³•å……åˆ†æ•æ‰è¯æ±‡ä¹‹é—´çš„è¯­ä¹‰å…³ç³»å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¦‚æœä»é›¶å¼€å§‹è®­ç»ƒè¯å‘é‡ï¼Œæ„å‘³ç€éœ€è¦åˆ†é…æ›´å¤šçš„è®¡ç®—èµ„æºè®©æ¨¡å‹å­¦ä¹ è¯æ±‡çš„è¡¨ç¤ºï¼Œå®¹æ˜“æ‹–æ…¢è®­ç»ƒè¿›åº¦ã€‚

æ‰€ä»¥åœ¨æœ¬æ¬¡å®éªŒä¸­æˆ‘é€‰æ‹©ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡å¯¹è¯å‘é‡è¿›è¡Œåˆå§‹åŒ–ã€‚ä¸ºäº†ä¿è¯ä¸­è‹±æ–‡è¯å‘é‡è¯­ä¹‰æ¥è¿‘ä»è€Œæ‹¥æœ‰æ›´å¥½çš„è®­ç»ƒæ•ˆæœ,ä¸­è‹±æ–‡è¯å‘é‡éƒ½é€‰ç”¨äº†è…¾è®¯AI labçš„å¼€æºçš„200ç»´é¢„è®­ç»ƒè¯å‘é‡(https://ai.tencent.com/ailab/nlp/en/embedding.html)è¿›è¡Œåˆå§‹åŒ–.

#### è¯»å–é¢„è®­ç»ƒè¯å‘é‡

ç”±äºä¸­è‹±æ–‡è¯å‘é‡åŠ è½½è¿‡ç¨‹ç›¸ä¼¼,è¿™é‡Œåªå±•ç¤ºè‹±æ–‡é¢„è®­ç»ƒè¯å‘é‡çš„åŠ è½½è¿‡ç¨‹.

è…¾è®¯çš„é¢„è®­ç»ƒè¯å‘é‡å®˜ç½‘ä¸­ç»™å‡ºçš„å°å‹è‹±æ–‡æ•°æ®é›†è¯æ±‡æ•°é‡ä¸ºä¸¤ç™¾ä¸‡ï¼Œä¸ºäº†ç¼©å‡è¯»å–æ—¶é•¿ï¼Œè¿™é‡Œåªè¯»å–è¯é¢‘æœ€é«˜çš„å‰50ä¸‡ä¸ªé¢„è®­ç»ƒè¯å‘é‡ï¼š

```python
def load_embeddings(glove_file_path, k):
    embeddings_index = {}
    with open(glove_file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= k:  # åªè¯»å–å‰kä¸ªè¯
                break
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs
    return embeddings_index

eng_embeddings = load_embeddings('data/word2vec/tencent-ailab-embedding-en-d200-v0.1.0-s.txt', k=500000)
```

#### åˆ›å»ºé¢„è®­ç»ƒè¯å‘é‡å­—å…¸

æ ¹æ®`eng_embeddings`åˆ›å»ºè¯å‘é‡å­—å…¸ï¼Œåœ¨`output_lang`çš„è¯æ±‡è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„è¯å‘é‡ï¼Œä¸åœ¨çš„ç”¨éšæœºåˆå§‹åŒ–çš„è¯å‘é‡ä»£æ›¿ã€‚åˆå§‹åŒ–çš„éšæœºå‘é‡åœ¨åˆå§‹åŒ–çš„å€¼åœ¨$-\sqrt{\frac{1}{dim}}$å’Œ$\sqrt{\frac{1}{dim}}$ä¹‹é—´å‡åŒ€é‡‡æ ·ï¼š

```python
output_embeddings_dim = eng_embeddings['1'].shape[0]
output_lang_embeddings = {}
# éå†output_langçš„æ‰€æœ‰tokenï¼Œåœ¨eng_embeddingsä¸­æŸ¥æ‰¾å¯¹åº”çš„è¯å‘é‡
for word in output_lang.word2index.keys():
    if word in eng_embeddings:
        output_lang_embeddings[word] = eng_embeddings[word]
    else:
        # åœ¨-sqrt(1/dim)å’Œsqrt(1/dim)ä¹‹é—´å‡åŒ€é‡‡æ ·
        output_lang_embeddings[word] = np.random.uniform(-np.sqrt(1/output_embeddings_dim), np.sqrt(1/output_embeddings_dim), output_embeddings_dim)
```

#### è½¬æ¢ç»´åµŒå…¥çŸ©é˜µ

ç”±äºåœ¨æ­£å¼è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œ`nn.embedding`æ˜¯é€šè¿‡åŠ è½½`torch`å¼ é‡çš„å½¢å¼è¯»å–é¢„è®­ç»ƒè¯å‘é‡çš„ï¼Œæ‰€ä»¥è¿˜éœ€è¦å°†è¯å‘é‡å­—å…¸è½¬æ¢æˆ`torch`å½¢å¼å¤‡ç”¨ï¼š

```python
# å°†è¯å‘é‡å­—å…¸è½¬æ¢ä¸ºåµŒå…¥çŸ©é˜µ
def create_embeddings_matrix(embeddings, lang):
    embeddings_matrix = np.zeros((lang.n_words, embeddings['1'].shape[0]))
    for word, idx in lang.word2index.items():
        embeddings_matrix[idx] = embeddings[word]
    return torch.FloatTensor(embeddings_matrix)

output_lang_embeddings_matrix = create_embeddings_matrix(output_lang_embeddings, output_lang)
```

## æ¨¡å‹æ¶æ„è®¾è®¡

### åŸºæœ¬å•å…ƒ

æ¨¡å‹ç¼–ç å™¨å’Œè§£ç å™¨éƒ½åŸºäº`RNN`æ¶æ„å®ç°ï¼Œå…¶åŸºæœ¬å•å…ƒéƒ½é€‰ç”¨äº†æ¯”LSTMè¡¨ç°æ›´å¥½çš„é—¨æ§å¾ªç¯å•å…ƒ`GRU`ï¼Œé€šè¿‡å¼•å…¥æ›´æ–°é—¨å’Œé‡ç½®é—¨æ¥æ§åˆ¶ä¿¡æ¯çš„æµåŠ¨ï¼Œä»è€Œè§£å†³é•¿æœŸä¾èµ–é—®é¢˜ã€‚æ›´æ–°é—¨å†³å®šäº†å‰ä¸€æ—¶åˆ»çš„ä¿¡æ¯æœ‰å¤šå°‘éœ€è¦ä¿ç•™åˆ°å½“å‰æ—¶åˆ»ï¼Œè€Œé‡ç½®é—¨åˆ™æ§åˆ¶äº†å‰ä¸€æ—¶åˆ»çš„ä¿¡æ¯å¯¹å½“å‰è¾“å…¥çš„å½±å“ç¨‹åº¦ï¼Œæ•´ä½“ç»“æ„æ¯”LSTMæ›´ç®€å•ä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚

ï¼ˆå›¾ç‰‡æ¥è‡ªææ²è€å¸ˆçš„`Dive into Deep Learning`æ•™ç¨‹æ–‡æ¡£ï¼‰

<img src="./assets/image-20240708194053198.png" alt="image-20240708194053198" style="zoom:50%;" />

åœ¨ä»£ç å®ç°ä¸­ï¼Œ`GRU`é€šè¿‡è°ƒç”¨`torch.nn`åº“ä¸­çš„`GRU`ç±»æ¥è¿›è¡Œå®ç°å¹¶ä¸”å±‚æ•°æŒ‡å®šä¸º2ï¼ŒåŒæ—¶æ ¹æ®å±‚æ•°è¿›è¡Œä¸€å®šç¨‹åº¦çš„`dropout`å¤„ç†é˜²æ­¢è¿‡æ‹Ÿåˆï¼š

```python
nn.GRU(hidden_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_p if num_layers > 1 else 0)
```

### Encoder

`EncodeRNN`å®ç°äº†ä¸€ä¸ªåŸºäº`GRU`çš„ç¼–ç å™¨ã€‚åœ¨åˆå§‹åŒ–æ—¶åµŒå…¥å±‚ä½¿ç”¨åœ¨æ•°æ®é¢„å¤„ç†æ—¶å‡†å¤‡å¥½çš„**é¢„è®­ç»ƒè¯å‘é‡**ï¼Œè€Œä¸”å…è®¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œå¾®è°ƒã€‚

åµŒå…¥å±‚åçš„åºåˆ—è¾“å…¥`GRU`å±‚è¿›è¡Œå¤„ç†ï¼ŒGRUå±‚åŒ…å«æŒ‡å®šæ•°é‡çš„å±‚æ•°ï¼Œå¹¶åœ¨å¤šå±‚æƒ…å†µä¸‹åº”ç”¨Dropouté˜²æ­¢è¿‡æ‹Ÿåˆã€‚åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè¾“å…¥çš„å•è¯ç´¢å¼•é¦–å…ˆé€šè¿‡åµŒå…¥å±‚å’ŒDropoutå±‚ï¼Œç„¶åç»è¿‡GRUå±‚ï¼Œæœ€ç»ˆå¾—åˆ°è¾“å‡º`output`å’Œéšè—çŠ¶æ€`hidden`ã€‚

```python
# ç¼–ç å™¨ï¼Œå°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºéšè—çŠ¶æ€ï¼Œä¾›è§£ç å™¨è¿›ä¸€æ­¥å¤„ç†
class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size, embedding_matrix, num_layers=2, dropout_p=0.1):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡åˆå§‹åŒ–åµŒå…¥å±‚
        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)
        # GRUå±‚ï¼Œç”¨äºå¤„ç†åµŒå…¥å‘é‡åºåˆ—
        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_p if num_layers > 1 else 0)
        # Dropoutå±‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, input):
        # å°†è¾“å…¥å•è¯ç´¢å¼•é€šè¿‡åµŒå…¥å±‚å’ŒDropoutå±‚
        embedded = self.dropout(self.embedding(input))
        # å°†åµŒå…¥å‘é‡åºåˆ—è¾“å…¥GRUå±‚ï¼Œå¾—åˆ°è¾“å‡ºå’Œéšè—çŠ¶æ€
        output, hidden = self.gru(embedded)
        
        # è¿”å›GRUçš„è¾“å‡ºå’Œæœ€åçš„éšè—çŠ¶æ€
        return output, hidden
```

å…¶ä¸­è¾“å‡ºä¸ºå¤„ç†å®Œè¾“å…¥åºåˆ—åäº§ç”Ÿçš„æ‰€æœ‰æ—¶é—´æ­¥ï¼ˆå³å¥å­ä¸­çš„æ¯ä¸ªtokenï¼‰çš„éšè—çŠ¶æ€ï¼Œå…¶å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼›è€Œéšè—çŠ¶æ€æ˜¯åœ¨å¤„ç†å®Œæ•´ä¸ªè¾“å…¥åºåˆ—åäº§ç”Ÿçš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚å…¶å½¢çŠ¶ä¸º `(num_layers, batch_size, hidden_size)`ã€‚

### æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°

åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­æˆ‘é€šè¿‡å®ç°ä¸åŒçš„å¯¹é½å‡½æ•°ï¼ˆ`Dot Product`ã€`Multiplicative`ã€`additive`ï¼‰å®ç°äº†ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶ã€‚

#### Dot Product Attention

Dot Product Attentionï¼Œå³ç‚¹ç§¯æ³¨æ„åŠ›ã€‚é€šè¿‡ç›´æ¥è®¡ç®— `query` å’Œ `keys` ä¹‹é—´çš„ç‚¹ç§¯æ¥è·å¾—æ³¨æ„åŠ›å€¼ï¼Œç®€å•é«˜æ•ˆã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š

1. **è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**ï¼šé€šè¿‡å°† `query` ä¸ `keys` è¿›è¡Œç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•° `scores`ã€‚
   $$
   \text{scores} = \text{query} \cdot \text{keys}^T
   $$

2. **è®¡ç®—æ³¨æ„åŠ›æƒé‡**ï¼šå¯¹æ³¨æ„åŠ›åˆ†æ•° `scores` è¿›è¡Œ softmax æ“ä½œï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ `weights`
   $$
   \text{weights} = \text{softmax}(\text{scores})
   $$

3. **è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡**ï¼šå°†æ³¨æ„åŠ›æƒé‡ `weights` ä¸ `keys` ç›¸ä¹˜ï¼Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡ `context`ã€‚
   $$
   \text{context} = \text{weights} \cdot \text{keys}
   $$

å®ç°ä»£ç å¦‚ä¸‹ï¼š

```python
class DotProductAttention(nn.Module):
    def __init__(self):
        super(DotProductAttention, self).__init__()

    def forward(self, query, keys):
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.bmm(query, keys.transpose(1, 2))
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        weights = F.softmax(scores, dim=-1)
        # è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡
        context = torch.bmm(weights, keys)

        return context, weights
```

#### Multiplicative Attention

Multiplicative Attentionï¼Œä¹Ÿå³ä¹˜æ³•æ³¨æ„åŠ›ã€‚ä¸ç‚¹ç§¯æ³¨æ„åŠ›ç±»ä¼¼ï¼Œä½†å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„çº¿æ€§å˜æ¢çŸ©é˜µ `W`ï¼Œç”¨æ¥å¯¹ `query` è¿›è¡Œå˜æ¢ï¼Œå†è®¡ç®—ä¸ `keys` çš„ç‚¹ç§¯ï¼Œå³ï¼š
$$
\text{transformed\_query} = W \cdot \text{query}\\
\text{scores} = \text{transformed\_query} \cdot \text{keys}^T 
$$
å®ç°ä»£ç å¦‚ä¸‹ï¼š
```python
class MultiplicativeAttention(nn.Module):
    def __init__(self, hidden_size):
        super(MultiplicativeAttention, self).__init__()
        self.W = nn.Linear(hidden_size, hidden_size, bias=False)  # å®šä¹‰çº¿æ€§å±‚W

    def forward(self, query, keys):
        # é€šè¿‡çº¿æ€§å±‚Wå¯¹queryè¿›è¡Œå˜æ¢
        transformed_query = self.W(query)
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.bmm(transformed_query, keys.transpose(1, 2))
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        weights = F.softmax(scores, dim=-1)
        # è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡
        context = torch.bmm(weights, keys)

        return context, weights
```

#### Bahdanau Attention

Bahdanau æ³¨æ„åŠ›ä¹Ÿå«Additive æ³¨æ„åŠ›ï¼Œåœ¨å‰é¢ä¸¤ä¸ªæ³¨æ„åŠ›æœºåˆ¶çš„åŸºç¡€ä¸Šè¿›è¡Œæ›´åŠ å¤æ‚çš„æ”¹è¿›ï¼Œå¼•å…¥äº†å‰é¦ˆç¥ç»ç½‘ç»œå°† `query` å’Œ `keys` è¿›è¡Œå˜æ¢å†è®¡ç®—å®ƒä»¬çš„ç›¸ä¼¼åº¦ï¼Œé€šè¿‡å¦ä¸€ä¸ªçº¿æ€§å±‚ `Va` å˜æ¢ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•° `scores`ã€‚å³ï¼š
$$
 \text{scores} = \text{Va}(\text{tanh}(\text{Wa}(\text{query}) + \text{Ua}(\text{keys})))
$$
ä»£ç å¦‚ä¸‹ï¼š

```python
class BahdanauAttention(nn.Module):
    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()
        self.Wa = nn.Linear(hidden_size, hidden_size)  # å®šä¹‰çº¿æ€§å±‚Wa
        self.Ua = nn.Linear(hidden_size, hidden_size)  # å®šä¹‰çº¿æ€§å±‚Ua
        self.Va = nn.Linear(hidden_size, 1)  # å®šä¹‰çº¿æ€§å±‚Va

    def forward(self, query, keys):
        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))  # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = scores.squeeze(2).unsqueeze(1)  # è°ƒæ•´scoresçš„å½¢çŠ¶

        weights = F.softmax(scores, dim=-1)  # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        context = torch.bmm(weights, keys)  # è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡

        return context, weights  # è¿”å›ä¸Šä¸‹æ–‡å‘é‡å’Œæ³¨æ„åŠ›æƒé‡
```

#### ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶çš„å¯¹æ¯”

å¯¹æ¯”å…¶ä»–ä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”±äºAdditiveæ³¨æ„åŠ›å¼•å…¥éçº¿æ€§å˜æ¢ï¼Œæ‹¥æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼ŒåŒæ—¶èƒ½å¤Ÿå¤„ç†ä¸åŒç»´åº¦çš„ `query` å’Œ `keys`ï¼Œæ›´åŠ çµæ´»ï¼Œè™½ç„¶å…¶è®¡ç®—å¼€é”€æ›´å¤§ï¼ˆ**ç›¸æ¯”ä¹‹ä¸‹ç‚¹ç§¯æ³¨æ„åŠ›åªéœ€è¦è®¡ç®—çŸ©é˜µä¹˜æ³•å³å¯ï¼Œè€ŒAdditive æ³¨æ„åŠ›éœ€è¦è®¡ç®—ä¸€å±‚å‰é¦ˆç¥ç»ç½‘ç»œ**ï¼‰ï¼Œä½†æ˜¯å…¶æ€§èƒ½è¦æ¯”å…¶ä»–ä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶è¦æ›´å¥½ã€‚

ä¸ºäº†ä»¤æ¨¡å‹èƒ½æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œæœ€ç»ˆç‰ˆæœ¬çš„ä»£ç ä¸­é€‰ç”¨Additiveæ³¨æ„åŠ›è¿›è¡Œæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—ã€‚

### Decoder

`AttnDecoderRNN`å®ç°äº†å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„åŒå±‚GRU Decoderã€‚åˆ©ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æ¥æ›´æœ‰æ•ˆåœ°å°†ç¼–ç å™¨ç”Ÿæˆçš„**ä¸Šä¸‹æ–‡å‘é‡**ï¼ˆå³Encoderä¸­è¾“å‡ºçš„`outputs`ï¼‰è½¬æ¢ä¸ºç›®æ ‡åºåˆ—ã€‚

#### Decoderæ¶æ„

DecoderåŒ…å«ä¸€ä¸ªåµŒå…¥å±‚ã€ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶ã€ä¸€ä¸ªGRUå±‚å’Œä¸€ä¸ªè¾“å‡ºçº¿æ€§å±‚ã€‚åµŒå…¥å±‚å°†è¾“å…¥çš„è¯æ±‡ç´¢å¼•è½¬æ¢ä¸ºå¯†é›†çš„å‘é‡è¡¨ç¤ºï¼Œæ³¨æ„åŠ›æœºåˆ¶ä½¿ç”¨äº†`Additive `æ³¨æ„åŠ›ï¼Œè®¡ç®—å½“å‰æ—¶é—´æ­¥çš„ä¸Šä¸‹æ–‡å‘é‡ï¼ŒGRUå±‚é€šè¿‡è¾“å…¥åµŒå…¥å‘é‡å’Œä¸Šä¸‹æ–‡å‘é‡çš„æ‹¼æ¥æ¥ç”Ÿæˆè¾“å‡ºï¼Œè¾“å‡ºå±‚å°†GRUå±‚çš„è¾“å‡ºæ˜ å°„åˆ°ç›®æ ‡è¯æ±‡è¡¨çš„å¤§å°ï¼Œè€ŒDropoutå±‚åˆ™ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

åœ¨Deocderä¸­ä¸åŒçš„æ˜¯**ç”±äºè¦è¾“å…¥Encoderä¸­è¾“å‡ºçš„æ¯ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä½œä¸ºä¸Šä¸‹æ–‡å‘é‡**ï¼Œæ‰€ä»¥è¿™é‡Œ`GRU`çš„è¾“å…¥ç»´åº¦ä¸º**è¯å‘é‡ç»´åº¦ä¹˜ä»¥2ï¼š**

![image-20240708210738751](./assets/image-20240708210738751.png)

```python
class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size, embedding_matrix, num_layers=2, dropout_p=0.1):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # å®šä¹‰åµŒå…¥å±‚
        self.attention = BahdanauAttention(hidden_size)  # å®šä¹‰æ³¨æ„åŠ›æœºåˆ¶
        self.gru = nn.GRU(2 * hidden_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_p if num_layers > 1 else 0)  # å®šä¹‰GRUå±‚
        self.out = nn.Linear(hidden_size, output_size)  # å®šä¹‰è¾“å‡ºçº¿æ€§å±‚
        self.dropout = nn.Dropout(dropout_p)  # å®šä¹‰Dropoutå±‚
```

#### Decoderå‰å‘ä¼ æ’­è¿‡ç¨‹

åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼ŒDecoderçš„è¾“å…¥åˆå§‹åŒ–ä¸ºå¼€å§‹æ ‡è®°ï¼ˆ`SOS_token`ï¼‰ï¼Œå…¶éšè—çŠ¶æ€åˆå§‹åŒ–ä¸ºç¼–ç å™¨çš„éšè—çŠ¶æ€ã€‚Decoderåœ¨æ¯ä¸ªæ—¶é—´æ­¥è¿›è¡Œä¸€æ­¥è§£ç ï¼Œé€šè¿‡forward_stepæ–¹æ³•è·å–å½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºå’Œæ³¨æ„åŠ›æƒé‡ã€‚è¿™ä¸ªæ–¹æ³•åŒ…æ‹¬é€šè¿‡åµŒå…¥å±‚è·å–è¾“å…¥çš„åµŒå…¥è¡¨ç¤ºï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ï¼Œå°†åµŒå…¥è¡¨ç¤ºå’Œä¸Šä¸‹æ–‡å‘é‡è¿æ¥åä¼ é€’ç»™GRUå±‚ï¼Œå¹¶é€šè¿‡çº¿æ€§å±‚ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚æ•´ä¸ªè¿‡ç¨‹åœ¨æ¯ä¸ªæ—¶é—´æ­¥å¾ªç¯è¿›è¡Œï¼Œç›´åˆ°ç”Ÿæˆå®Œæ•´çš„ç›®æ ‡åºåˆ—ï¼Œå¹¶å°†æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºå’Œæ³¨æ„åŠ›æƒé‡åˆ†åˆ«å­˜å‚¨èµ·æ¥ã€‚

```python
def forward(self, encoder_outputs, encoder_hidden, target_tensor=None, isTeacherForcing = True):
        if not isTeacherForcing:
            target_tensor = None
        batch_size = encoder_outputs.size(0)  
        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)  # åˆå§‹åŒ–è§£ç å™¨çš„è¾“å…¥ä¸ºSOS_token
        decoder_hidden = encoder_hidden 
        decoder_outputs = []  
        attentions = []  

        for i in range(MAX_LENGTH):
            decoder_output, decoder_hidden, attn_weights = self.forward_step(   
                decoder_input, decoder_hidden, encoder_outputs
            )
            decoder_outputs.append(decoder_output)  
            attentions.append(attn_weights)  

            if target_tensor is not None:
                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing
            else:
                _, topi = decoder_output.topk(1)		# greedyç­–ç•¥
                decoder_input = topi.squeeze(-1).detach()  # ä½¿ç”¨è‡ªå·±çš„é¢„æµ‹ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥

        decoder_outputs = torch.cat(decoder_outputs, dim=1)  
        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)  
        attentions = torch.cat(attentions, dim=1)  

        return decoder_outputs, decoder_hidden, attentions  

def forward_step(self, input, hidden, encoder_outputs):
    embedded = self.dropout(self.embedding(input)) 

    query = hidden.permute(1, 0, 2)  
    context, attn_weights = self.attention(query, encoder_outputs)  
    input_gru = torch.cat((embedded, context), dim=2)  

    output, hidden = self.gru(input_gru, hidden) 
    output = self.out(output)  

    return output, hidden, attn_weights  # è¿”å›è¾“å‡ºã€éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æƒé‡
```

#### è®­ç»ƒæ–¹æ³•çš„å®ç°

åœ¨Decoderçš„å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­å¯ä»¥è¿›è¡Œ`Teacher Forcing`ä»¥åŠ`Free Running`ä¸¤ç§ç­–ç•¥çš„é€‰æ‹©ï¼Œå½“é€‰æ‹©Teacher Forcingç­–ç•¥æ—¶ï¼Œæ¨¡å‹ä¼šé€‰æ‹©Ground Truthæ ‡ç­¾å¯¹ç¼–ç å™¨è¿›è¡Œè®­ç»ƒï¼›è€Œå¦‚æœé€‰æ‹©Free Runningç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œåˆ™æ¨¡å‹ä¼šä½¿ç”¨è‡ªå·±ä¸Šä¸€æ—¶é—´æ­¥ç”Ÿæˆçš„è¾“å‡ºä½œä¸ºä¸‹ä¸€æ—¶é—´æ­¥çš„è¾“å…¥å¯¹Decoderè¿›è¡Œè®­ç»ƒã€‚ä¸€èˆ¬æƒ…å†µä¸‹è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥ä»»æ„é€‰æ‹©ä¸¤ç§ç­–ç•¥ï¼ˆç”šè‡³å¯ä»¥æŒ‡å®šä¸€å®šçš„Teacher Forcingæ¯”ä¾‹ï¼‰ï¼Œè€Œé¢„æµ‹è¿‡ç¨‹ä¸­åªèƒ½è¿›è¡ŒFree Runningã€‚

```python
        if not isTeacherForcing:
            target_tensor = None
        if target_tensor is not None:
            decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing
        else:
            _, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze(-1).detach()  # ä½¿ç”¨è‡ªå·±çš„é¢„æµ‹ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥
```

åœ¨åé¢çš„å®éªŒéƒ¨åˆ†ä¸­æˆ‘å¯¹ä¸¤ç§æ–¹æ³•çš„è®­ç»ƒæ”¶æ•›é€Ÿç‡è¿›è¡Œäº†ç›´æ¥å¯¹æ¯”ï¼Œå‘ç°**Teacher Forcingçš„æ”¶æ•›é€Ÿåº¦è¦æ¯”Free Runningå¿«å¾—å¤šï¼Œè€Œä¸”æœ€ç»ˆçš„æŸå¤±å€¼æ›´ä½ã€æ”¶æ•›æ•ˆæœæ›´å¥½**ã€‚

#### è§£ç ç­–ç•¥çš„å®ç°

##### è´ªå¿ƒæœç´¢ greedy

ä¸Šé¢çš„ä»£ç å®ç°çš„æ˜¯greedyè´ªå¿ƒè§£ç ç­–ç•¥ï¼Œæ„åœ¨äºè¾“å‡ºåºåˆ—çš„æ¯ä¸€æ—¶é—´æ­¥ğ‘¡â€²ï¼Œ æˆ‘ä»¬éƒ½å°†åŸºäºè´ªå¿ƒæœç´¢ä»ğ‘Œä¸­æ‰¾åˆ°å…·æœ‰æœ€é«˜æ¡ä»¶æ¦‚ç‡çš„è¯å…ƒï¼Œ

```python
_, topi = decoder_output.topk(1)
decoder_input = topi.squeeze(-1).detach()
```

é‚£ä¹ˆè´ªå¿ƒæœç´¢ä¹Ÿå­˜åœ¨å¾ˆæ˜æ˜¾çš„é—®é¢˜ï¼š ç°å®ä¸­ï¼Œæœ€ä¼˜åºåˆ—åº”è¯¥æ˜¯æœ€å¤§åŒ–
$$
\prod_{t'=1}^{T'} P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})
$$
å€¼çš„è¾“å‡ºåºåˆ—ï¼Œè¿™æ˜¯åŸºäºè¾“å…¥åºåˆ—ç”Ÿæˆè¾“å‡ºåºåˆ—çš„æ¡ä»¶æ¦‚ç‡ã€‚ ç„¶è€Œè´ªå¿ƒæœç´¢æ˜¯æ— æ³•ä¿è¯å¾—åˆ°æœ€ä¼˜åºåˆ—çš„ã€‚

#### æŸæœç´¢beam-search

ç”±äºæ¯ä¸€æ­¥éƒ½ç›´æ¥æœç´¢å½“å‰æ¦‚ç‡æœ€é«˜çš„ç»“æœï¼Œè´ªå¿ƒç­–ç•¥æ— ç–‘æ˜¯é€Ÿåº¦æœ€å¿«çš„ï¼Œä½†æ˜¯æ•ˆæœæ˜¾ç„¶ä¼šç•¥å·®ã€‚**è€ŒæŸæœç´¢åˆ™åœ¨ç‰ºç‰²äº†ä¸€å®šè®¡ç®—é‡çš„åŒæ—¶ï¼Œå¢åŠ äº†æœç´¢ç²¾åº¦å’Œæ•ˆæœã€‚**

æŸæœç´¢æ˜¯è´ªå¿ƒæœç´¢çš„ä¸€ä¸ªæ”¹è¿›ç‰ˆæœ¬ã€‚ æœ‰ä¸€ä¸ªè¶…å‚æ•°æŸå®½$ğ‘˜$ã€‚ åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥**éƒ½é€‰æ‹©å…·æœ‰æœ€é«˜æ¡ä»¶æ¦‚ç‡çš„$ğ‘˜$ä¸ªè¯å…ƒ**ã€‚ è¿™ğ‘˜ä¸ªè¯å…ƒå°†åˆ†åˆ«æ˜¯ğ‘˜ä¸ªå€™é€‰è¾“å‡ºåºåˆ—çš„ç¬¬ä¸€ä¸ªè¯å…ƒã€‚ åœ¨éšåçš„**æ¯ä¸ªæ—¶é—´æ­¥ï¼ŒåŸºäºä¸Šä¸€æ—¶é—´æ­¥çš„$ğ‘˜$ä¸ªå€™é€‰è¾“å‡ºåºåˆ—**ï¼Œ å°†ç»§ç»­ä»$ğ‘˜|ğ‘Œ|$ä¸ªå¯èƒ½çš„é€‰æ‹©ä¸­ æŒ‘å‡ºå…·æœ‰æœ€é«˜æ¡ä»¶æ¦‚ç‡çš„$ğ‘˜$â€‹ä¸ªå€™é€‰è¾“å‡ºåºåˆ—ã€‚

ï¼ˆå›¾ç‰‡æ¥è‡ªææ²è€å¸ˆçš„`Dive into Deep Learning`æ•™ç¨‹æ–‡æ¡£ï¼‰

<img src="./assets/image-20240708204849478.png" alt="image-20240708204849478" style="zoom:50%;" />

æœ€åé€‰æ‹©å…¶ä¸­æ¡ä»¶æ¦‚ç‡ä¹˜ç§¯æœ€é«˜çš„åºåˆ—ä½œä¸ºè¾“å‡ºåºåˆ—ï¼š
$$
\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c}),
$$
å®ç°ä»£ç å¦‚ä¸‹ï¼š

```python
 def beam_search_decoding(self, encoder_outputs, decoder_hidden, beam_width):
        batch_size = encoder_outputs.size(0)
        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=encoder_outputs.device).fill_(SOS_token)

        sequences = [[[list(), 0.0, decoder_hidden, decoder_input]]] * batch_size  # Initialize with start token

        for _ in range(MAX_LENGTH):
            all_candidates = [[] for _ in range(batch_size)]
            for i in range(batch_size):
                for seq, score, hidden, input_token in sequences[i]:
                    output, hidden, _ = self.forward_step(input_token, hidden, encoder_outputs[i:i+1])
                    log_probs, indices = torch.topk(F.log_softmax(output, dim=-1), beam_width)
                    for log_prob, idx in zip(log_probs[0, 0], indices[0, 0]):
                        candidate = [seq + [idx.item()], score + log_prob.item(), hidden, idx.view(1, 1)]
                        all_candidates[i].append(candidate)

            sequences = [sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width] for candidates in all_candidates]

        decoded_sequences = []
        for i in range(batch_size):
            decoded_sequences.append(sequences[i][0][0])  # é€‰æ‹©å‰kä¸ªæœ€ä½³é€‰æ‹©

        return torch.tensor(decoded_sequences, dtype=torch.long, device=encoder_outputs.device), None, None
```

ä¸ºäº†ä¿è¯æ¨¡å‹çš„è®­ç»ƒæ•ˆæœï¼Œæœ€ç»ˆç‰ˆæœ¬çš„ä»£ç ä¸­é€‰æ‹©`æŸæœç´¢`ç­–ç•¥è¿›è¡Œè§£ç ã€‚

## è®­ç»ƒå’Œæ¨ç†

### æ•°æ®é›†Dataloaderçš„æ„å»º

`get_dataloader`é¦–å…ˆè°ƒç”¨`prepareData`å‡½æ•°å‡†å¤‡æ•°æ®é›†ã€‚ç„¶åå°†æ¯ä¸ªå¥å­å¯¹è½¬æ¢ä¸ºç´¢å¼•åˆ—è¡¨ï¼Œå¹¶åœ¨åˆ—è¡¨æœ«å°¾æ·»åŠ ç»“æŸæ ‡è®°ï¼ˆEOS_tokenï¼‰åˆ›å»ºTensorDatasetå¯¹è±¡ã€‚

æœ€åä½¿ç”¨`RandomSampler`éšæœºæŠ½å–æ ·æœ¬ï¼ŒæŒ‡å®šçš„æ‰¹é‡å¤§å°åˆ›å»ºDataLoaderå¯¹è±¡ã€‚è¿”å›æºè¯­è¨€åºåˆ—ä»¥åŠç›®æ ‡è¯­è¨€åºåˆ—çš„`lang`ç±»å’Œè®­ç»ƒæ—¶è¿›è¡Œè¿­ä»£çš„DataLoaderå¯¹è±¡ã€‚

```python
"""data loader"""
def get_dataloader(batch_size, text_type='train'):
 	# è°ƒç”¨prepareDataå‡½æ•°ï¼Œè·å–è¾“å…¥è¯­è¨€å¯¹è±¡ã€è¾“å‡ºè¯­è¨€å¯¹è±¡å’Œå¥å­å¯¹åˆ—è¡¨
    input_lang, output_lang, pairs = prepareData('eng', 'zh', True, text_type)
   
	# åˆå§‹åŒ–ç”¨äºå­˜å‚¨è¾“å…¥å’Œç›®æ ‡å¥å­ç´¢å¼•çš„æ•°ç»„ï¼Œå½¢çŠ¶ä¸ºï¼ˆå¥å­å¯¹æ•°é‡, æœ€å¤§å¥å­é•¿åº¦ï¼‰
    n = len(pairs)
    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)
    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)
    

    for idx, (inp, tgt) in enumerate(pairs):        
        inp_ids = indexesFromSentence(input_lang, inp)
        tgt_ids = indexesFromSentence(output_lang, tgt)
		# åœ¨æ¯ä¸ªç´¢å¼•åˆ—è¡¨æœ«å°¾æ·»åŠ ç»“æŸæ ‡è®°EOS_token
        inp_ids.append(EOS_token)
        tgt_ids.append(EOS_token)
		# å°†ç´¢å¼•åˆ—è¡¨å¡«å……åˆ°æ•°ç»„ä¸­
        input_ids[idx, :len(inp_ids)] = inp_ids
        target_ids[idx, :len(tgt_ids)] = tgt_ids
        
    # åˆ›å»ºTensorDatasetå¯¹è±¡
    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),
                               torch.LongTensor(target_ids).to(device))
    train_sampler = RandomSampler(train_data)

    # åˆ›å»ºDataLoaderå¯¹è±¡
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

    # è¿”å›è¾“å…¥è¯­è¨€å¯¹è±¡ã€è¾“å‡ºè¯­è¨€å¯¹è±¡å’ŒDataLoaderå¯¹è±¡
    return input_lang, output_lang, train_dataloader
```

### è®­ç»ƒ

æŒ‡å®šè®­ç»ƒçš„epochsåå¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œ`n_epochs`æ¬¡è¿­ä»£ï¼š

```python
    for epoch in range(1, n_epochs + 1):
        # è®­ç»ƒä¸€ä¸ªepoch
        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)
        print_loss_total += loss  # ç´¯åŠ å½“å‰epochçš„æŸå¤±
        plot_loss_total += loss  # ç´¯åŠ å½“å‰epochçš„æŸå¤±
```

åœ¨æ¯ä¸ªè¿­ä»£epochä¸­ï¼Œé¦–å…ˆåˆå§‹åŒ–æ€»æŸå¤±ï¼Œç„¶åå¯¹ç¼–ç å™¨å’Œè§£ç å™¨çš„æ¢¯åº¦è¿›è¡Œæ¸…é›¶ã€‚å°†è½¬åŒ–ä¸ºå¼ é‡å½¢å¼çš„æºè¯­è¨€åºåˆ—é€å…¥`encoder`åè¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥ï¼ˆå³å¥å­ä¸­çš„æ¯ä¸ªtokenï¼‰çš„éšè—çŠ¶æ€å’Œå¤„ç†å®Œæ•´ä¸ªè¾“å…¥åºåˆ—åäº§ç”Ÿçš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼›ç„¶åå†å’Œç›®æ ‡è¯­è¨€åºåˆ—çš„å¼ é‡å½¢å¼ä¸€èµ·é€å…¥è§£ç å™¨è®¡ç®—å¾—åˆ°è¾“å‡ºã€‚æœ€åè®¡ç®—å½“å‰epochçš„æŸå¤±å¹¶åå‘ä¼ æ’­æ›´æ–°å‚æ•°ã€‚

æ¯ä¸€ä¸ªepochçš„è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š

```python
def train_epoch(dataloader, encoder, decoder, encoder_optimizer,
                decoder_optimizer, criterion):
    total_loss = 0  # åˆå§‹åŒ–æ€»æŸå¤±

    for data in dataloader:
        input_tensor, target_tensor = data  

        encoder_optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
        decoder_optimizer.zero_grad()  

        encoder_outputs, encoder_hidden = encoder(input_tensor)  # é€šè¿‡ç¼–ç å™¨
        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor, isTeacherForcing = True)  # é€šè¿‡è§£ç å™¨

        # è®¡ç®—æŸå¤±
        loss = criterion(
            decoder_outputs.view(-1, decoder_outputs.size(-1)),
            target_tensor.view(-1)
        )
        loss.backward()  # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦

        encoder_optimizer.step()  # æ›´æ–°å‚æ•°
        decoder_optimizer.step()  

        total_loss += loss.item()  # ç´¯åŠ æŸå¤±

    return total_loss / len(dataloader)  # è¿”å›å¹³å‡æŸå¤±
```

### é¢„æµ‹

é¢„æµ‹ä¸è®­ç»ƒåŸºæœ¬ä¸€è‡´ï¼Œä¸åŒçš„åœ°æ–¹åœ¨äºåªä½¿ç”¨FreeRunningç­–ç•¥ï¼Œå¹¶ä¸”æœ€ç»ˆå°†å¾—åˆ°çš„è¾“å‡ºè½¬æ¢ä¸ºç´¢å¼•å¹¶ä¸”æ ¹æ®è¯æ±‡è¡¨å¾—åˆ°å•è¯åºåˆ—å¹¶è¿”å›ã€‚

```python
def evaluate(encoder, decoder, sentence, input_lang, output_lang, beam_width=5):
    with torch.no_grad():
        input_tensor = tensorFromSentence(input_lang, sentence)
        # å‰å‘ä¼ æ’­
        encoder_outputs, encoder_hidden = encoder(input_tensor)
        # ä½¿ç”¨æŸæœç´¢å¹¶ä¸”ä½¿ç”¨FreeRuningç­–ç•¥
        decoded_sequences, decoder_hidden, decoder_attn = decoder.beam_search_decoding(encoder_outputs, encoder_hidden, beam_width)

        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åºåˆ—
        decoded_ids = decoded_sequences[0]

        # å°†ç´¢å¼•è½¬æ¢ä¸ºå•è¯
        decoded_words = []
        for idx in decoded_ids:
            if idx.item() == EOS_token:
                decoded_words.append('<EOS>')  # å¦‚æœé‡åˆ°ç»“æŸæ ‡è®°ï¼Œåˆ™åœæ­¢ç¿»è¯‘
                break
            decoded_words.append(output_lang.index2word[idx.item()])

    # è¿”å›ç¿»è¯‘çš„å•è¯åˆ—è¡¨å’Œæ³¨æ„åŠ›æƒé‡
    return decoded_words, decoder_attn
```

### BLEUè¯„ä¼°æ ‡å‡†çš„å®ç°

æœ¬æ¬¡å®éªŒä½¿ç”¨`BLEU`è¯„ä¼°æ ‡å‡†æ¥è¡¡é‡ç¿»è¯‘è´¨é‡ã€‚BLEUæ˜¯ä¸€ç§ç”¨äºè¯„ä¼°æœºå™¨ç¿»è¯‘ç³»ç»Ÿè¾“å‡ºè´¨é‡çš„æŒ‡æ ‡ã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡æ¯”è¾ƒæœºå™¨ç¿»è¯‘ç»“æœå’Œä¸€ä¸ªæˆ–å¤šä¸ªå‚è€ƒè¯‘æ–‡ä¹‹é—´çš„ n-gram ç²¾ç¡®åŒ¹é…ç¨‹åº¦æ¥è¯„ä¼°ç¿»è¯‘è´¨é‡ã€‚ä¸»è¦æ­¥éª¤åŒ…æ‹¬è®¡ç®— n-gram ç²¾ç¡®åº¦ã€æƒ©ç½šæœºåˆ¶å’Œç»¼åˆè¯„åˆ†ã€‚

#### è®¡ç®— n-gram ç²¾ç¡®åº¦

$$
P_n = \frac{\sum_{C \in \{Candidates\}} \sum_{ngram \in C} \min(\text{Count}_{clip}(ngram), \text{Count}_{ref}(ngram))}{\sum_{C \in \{Candidates\}} \sum_{ngram \in C} \text{Count}_{C}(ngram)}
$$

å…¶ä¸­ï¼Œ$\text{Count}_{clip}(ngram)$ æ˜¯å€™é€‰ç¿»è¯‘ä¸­ n-gram çš„æ•°é‡ï¼Œ$\text{Count}_{ref}(ngram)$$æ˜¯å‚è€ƒè¯‘æ–‡ä¸­ n-gram çš„æ•°é‡ã€‚

å®ç°ä»£ç å¦‚ä¸‹ï¼š

```python
def n_gram_precision(candidate, references, n):
    candidate_n_grams = Counter([tuple(candidate[i:i+n]) for i in range(len(candidate)-n+1)])
    references_n_grams = Counter(chain(*[Counter([tuple(reference[i:i+n]) for i in range(len(reference)-n+1)]) for reference in references]))

    clipped_count = {ngram: min(count, references_n_grams[ngram]) for ngram, count in candidate_n_grams.items()}
    precision = sum(clipped_count.values()) / max(sum(candidate_n_grams.values()), 1)

    return precision
```

#### è®¡ç®— Brevity Penaltyé•¿åº¦æƒ©ç½š

$$
BP = \begin{cases} 
   1 & \text{if } c > r \\
   e^{(1 - \frac{r}{c})} & \text{if } c \leq r 
   \end{cases}
$$

å…¶ä¸­ï¼Œ$c$æ˜¯å€™é€‰ç¿»è¯‘çš„é•¿åº¦ï¼Œ$r$â€‹æ˜¯å‚è€ƒè¯‘æ–‡çš„é•¿åº¦ã€‚

å®ç°ä»£ç å¦‚ä¸‹ï¼š

```python
def brevity_penalty(candidate, references):
    c = len(candidate)
    r = min((abs(len(reference) - c), len(reference)) for reference in references)[1]

    if c > r:
        return 1
    else:
        return math.exp(1 - r/c)
```

#### è®¡ç®— BLEU åˆ†æ•°

$$
\text{BLEU} = BP \cdot \exp \left( \sum_{n=1}^{N} w_n \log P_n \right)
$$

é€šå¸¸$w_n$æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œå³$w_n = \frac{1}{N}$â€‹ã€‚

ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
def BLEU(candidate, references, n=4):
    weights = [1/n] * n
    p_n = [n_gram_precision(candidate, references, i) for i in range(1, n+1)]
    score = brevity_penalty(candidate, references) * math.exp(sum(w * math.log(p) for w, p in zip(weights, p_n) if p > 0))

    return score
```

### å‚æ•°è®¾ç½®

åœ¨æœ¬æ¬¡å®éªŒä¸­é€‰æ‹©`batch_size`æ‰¹é‡å¤§å°ä¸º32

```python
batch_size = 32
```

å¹¶ä¸”é€‰æ‹©`Adam`ä½œä¸ºä¼˜åŒ–å™¨ï¼Œé€‰æ‹©`NLLOSS`è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¹¶ä¸”`å­¦ä¹ ç‡`è®¾ç½®ä¸º0.001ï¼š

```python
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)
  	# å®šä¹‰æŸå¤±å‡½æ•°
    criterion = nn.NLLLoss()
    learning_rate=0.001
```

## å®éªŒ

æœ¬æ¬¡å®éªŒä¸­æ€»å…±é€šè¿‡æ”¹å˜**æ¨¡å‹å±‚æ•°ã€æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡ã€è®­ç»ƒç­–ç•¥ã€æ•°æ®é›†å¤§å°**è¿›è¡Œäº†å¤šæ¬¡æ¨ªå‘å¯¹æ¯”ã€‚

ä¸åŒç»“æ„æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹é€šè¿‡`tensorboard`è®°å½•ã€‚æ•´ä½“å®éªŒç»“æœå¦‚ä¸‹ï¼š

| å±‚æ•° | é¢„è®­ç»ƒè¯å‘é‡ | è®­ç»ƒç­–ç•¥        | æ•°æ®é›†å¤§å° | è®­ç»ƒæ—¶é•¿     | æœ€ç»ˆlosså€¼ | æµ‹è¯•é›†BLEUå€¼      |
| ---- | ------------ | --------------- | ---------- | ------------ | ---------- | ----------------- |
| ä¸€å±‚ | å¦           | Teacher Forcing | 10k        | 2.722 hr     | 0.1145     | 0.01948(1.9%)     |
| ä¸€å±‚ | æ˜¯           | Teacher Forcing | 10k        | 2.853 hr     | **0.0131** | 0.02669(2.7%)     |
| ä¸€å±‚ | æ˜¯           | Running Free    | 10k        | **2.706 hr** | 1.1078     | 0.00934(0.9%)     |
| ä¸¤å±‚ | æ˜¯           | Teacher Forcing | 10k        | 3.782 hr     | 0.0616     | 0.02257(2.3%)     |
| ä¸¤å±‚ | æ˜¯           | Teacher Forcing | 100k       | 30.58 hr*    | 0.0616     | **0.03697(3.7%)** |

\*ç”±äºè®­ç»ƒæ—¶é—´è¿‡é•¿ï¼Œ100kçš„æ•°æ®é›†åªè·‘äº†ä¸åˆ°200ä¸ªEpochsã€‚

<img src="./assets/image-20240708221259484.png" alt="image-20240708221259484" style="zoom:80%;" />

### Teacher Forcingä¸Running Freeç­–ç•¥å¯¹æ¯”

| å±‚æ•° | é¢„è®­ç»ƒè¯å‘é‡ | è®­ç»ƒç­–ç•¥        | æ•°æ®é›†å¤§å° | è®­ç»ƒæ—¶é•¿     | æœ€ç»ˆlosså€¼ | æµ‹è¯•é›†BLEUå€¼  |
| ---- | ------------ | --------------- | ---------- | ------------ | ---------- | ------------- |
| ä¸€å±‚ | å¦           | Teacher Forcing | 10k        | 2.722 hr     | 0.1145     | 0.01948(1.9%) |
| ä¸€å±‚ | æ˜¯           | Running Free    | 10k        | **2.706 hr** | 1.1078     | 0.00934(0.9%) |

é€šè¿‡å›¾è¡¨å’Œè®­ç»ƒæ›²çº¿å¯ä»¥çœ‹åˆ°ï¼ŒTeacher Forcingæ‹¥æœ‰æ¯”Running Free**æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦**ï¼Œå› ä¸ºæ¨¡å‹æ€»æ˜¯æ¥æ”¶åˆ°æ­£ç¡®çš„è¾“å…¥ï¼Œæ¢¯åº¦æ›´ç¨³å®šï¼Œæ¨¡å‹å¯ä»¥æ›´å¿«åœ°å­¦ä¹ åˆ°æ­£ç¡®çš„åºåˆ—ç”Ÿæˆã€‚

åŒæ—¶ï¼ŒTeacher Forcingçš„è®­**ç»ƒä¹Ÿæ›´åŠ ç¨³å®šï¼ŒæŸå¤±å€¼æ›´ä½**ï¼Œå› ä¸ºTracherForcingä½¿ç”¨çœŸå®çš„ç›®æ ‡åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œè€ŒRuning Freeä½¿ç”¨è‡ªå·±é¢„æµ‹çš„å‰ä¸€ä¸ªè¾“å‡ºä½œä¸ºä¸‹ä¸€æ—¶åˆ»çš„è¾“å…¥ï¼Œå®¹æ˜“å¯¼è‡´**è¯¯å·®ç´¯ç§¯**ï¼Œä»¥è‡³äºè®­ç»ƒæ•ˆæœä¸ä½³ï¼Œåœ¨æ‰€æœ‰æ¨¡å‹æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€ä½çš„0.9%ã€‚

<img src="./assets/image-20240708221926117.png" alt="image-20240708221926117" style="zoom:80%;" />

è™½ç„¶æ ¹æ®ç†è®ºï¼Œä½¿ç”¨Teachering Forcingåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æ€»æ˜¯æ¥æ”¶åˆ°æ­£ç¡®çš„å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä½œä¸ºå½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ï¼Œè€Œåœ¨å®é™…çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¿…é¡»ä½¿ç”¨è‡ªå·±å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºã€‚ç”±æ­¤å¯èƒ½ä¼šäº§ç”Ÿ**æ›å…‰åå·®ï¼ˆExposure Biasï¼‰**ä¼šå¯¼è‡´æ¨¡å‹åœ¨æµ‹è¯•æ—¶è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å­¦ä¼šåœ¨é”™è¯¯ç´¯ç§¯çš„æƒ…å†µä¸‹è¿›è¡Œè°ƒæ•´ã€‚ä½†æ˜¯ç”±äºå®éªŒæ•°æ®é›†è¾ƒå°ï¼Œå¯¹æ¯”æµ‹è¯•åšçš„ä¹Ÿä¸å¤Ÿå……åˆ†ï¼Œåœ¨æœ¬æ¬¡å®éªŒä¸­æ²¡æœ‰ä½“ç°å‡ºæ¥ã€‚

### æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡

| å±‚æ•° | é¢„è®­ç»ƒè¯å‘é‡ | è®­ç»ƒç­–ç•¥        | æ•°æ®é›†å¤§å° | è®­ç»ƒæ—¶é•¿     | æœ€ç»ˆlosså€¼ | æµ‹è¯•é›†BLEUå€¼      |
| ---- | ------------ | --------------- | ---------- | ------------ | ---------- | ----------------- |
| ä¸€å±‚ | å¦           | Teacher Forcing | 10k        | **2.722 hr** | 0.1145     | 0.01948(1.9%)     |
| ä¸€å±‚ | æ˜¯           | Teacher Forcing | 10k        | 2.853 hr     | **0.0131** | **0.02669(2.7%)** |

é€šè¿‡å›¾è¡¨å’Œè®­ç»ƒæ›²çº¿å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡çš„æ¨¡å‹æ”¶æ•›é€Ÿåº¦æ¯”ä¸ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡çš„**æ”¶æ•›é€Ÿåº¦æ›´å¿«**ï¼Œè€Œä¸”æœ€ç»ˆçš„æŸå¤±å€¼è¿œä½äºä»é›¶å¼€å§‹è®­ç»ƒè¯å‘é‡çš„æƒ…å†µï¼Œè¿™æ˜¯å› ä¸ºé¢„è®­ç»ƒè¯å‘é‡å·²ç»åœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œè¿™äº›è¯å‘é‡å·²ç»å­¦ä¹ åˆ°äº†ä¸°å¯Œçš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åˆ°åŒä¹‰è¯ä¹‹é—´çš„å…³ç³»ä»¥åŠè¯ä¸è¯ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

è¿™æ„å‘³ç€åœ¨æ¨¡å‹è®­ç»ƒå¼€å§‹æ—¶ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡å·²ç»å…·å¤‡äº†è¾ƒé«˜è´¨é‡çš„åˆå§‹è¡¨ç¤ºï¼Œèƒ½æœ‰åŠ©äºæ¨¡å‹æ›´å¿«åœ°ç†è§£å’Œå¤„ç†è¯­è¨€æ•°æ®ï¼›è€Œä¸”é¢„è®­ç»ƒè¯å‘é‡æä¾›äº†æ›´ç¨³å®šçš„åˆå§‹æ¢¯åº¦ï¼Œå‡å°‘äº†è®­ç»ƒåˆæœŸçš„éœ‡è¡ã€‚

å†è€…ï¼Œ**ç”±äºä½¿ç”¨çš„æ•°æ®é›†å¯¹äºNLPä»»åŠ¡æ¥è¯´å®é™…ä¸Šéå¸¸å°**ï¼Œæ— è®ºå“ªä¸ªæ¨¡å‹éƒ½å­˜åœ¨ä¸¥é‡çš„è¿‡æ‹Ÿåˆç°è±¡ï¼Œè€Œé¢„è®­ç»ƒè¯å‘é‡ç”±äºåœ¨å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œ**å…·å¤‡äº†ä¸€å®šçš„æ³›åŒ–èƒ½åŠ›**ï¼Œèƒ½ä¸€å®šç¨‹åº¦å‡å°‘è¿‡æ‹Ÿåˆï¼Œåœ¨éªŒè¯é›†ä¸Šè¡¨ç°æ›´å¥½ï¼Œæ¯”ä¸ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡çš„æ¨¡å‹**å¢åŠ å¤§çº¦50%çš„æ€§èƒ½**ã€‚

<img src="./assets/image-20240708215558254.png" alt="image-20240708215558254" style="zoom: 80%;" />

### æ¨¡å‹å±‚æ•°

| å±‚æ•° | é¢„è®­ç»ƒè¯å‘é‡ | è®­ç»ƒç­–ç•¥        | æ•°æ®é›†å¤§å° | è®­ç»ƒæ—¶é•¿ | æœ€ç»ˆlosså€¼ | æµ‹è¯•é›†BLEUå€¼      |
| ---- | ------------ | --------------- | ---------- | -------- | ---------- | ----------------- |
| ä¸€å±‚ | æ˜¯           | Teacher Forcing | 10k        | 2.853 hr | **0.0131** | **0.02669(2.7%)** |
| ä¸¤å±‚ | æ˜¯           | Teacher Forcing | 10k        | 3.782 hr | 0.0616     | 0.02257(2.3%)     |

é€šè¿‡å›¾è¡¨å’Œè®­ç»ƒæ›²çº¿å¯ä»¥çœ‹åˆ°å¯¹äºä¸¤å±‚çš„æ¨¡å‹ç”±äºå¢åŠ äº†å‚æ•°é‡ï¼Œæ‰€ä»¥è®­ç»ƒæ—¶é•¿æ›´é•¿ã€‚ä½†æ˜¯æ­£å¦‚å‰é¢æåˆ°çš„ï¼Œç”±äºä½¿ç”¨çš„æ•°æ®é›†å¯¹äºNLPä»»åŠ¡æ¥è¯´å®é™…ä¸Šéå¸¸å°ï¼Œå¯¹äºè¾ƒå°çš„æ•°æ®é›†åœ¨å¢åŠ æ¨¡å‹å±‚æ•°æé«˜æ¨¡å‹å¤æ‚åº¦çš„æƒ…å†µä¸‹åªä¼šäº§ç”Ÿåä½œç”¨ã€‚

å¯ä»¥çœ‹åˆ°å¯¹æ¯”ä¸€å±‚çš„æ¨¡å‹ï¼Œä¸¤å±‚æ›´é«˜å¤æ‚åº¦æƒ…å†µä¸‹çš„æ¨¡å‹ä¸ä»…æœ€ç»ˆlosså€¼æ›´é«˜ï¼Œè€Œä¸”åœ¨æµ‹è¯•é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½ä¹Ÿä¸åŠä¸€å±‚çš„æ¨¡å‹ã€‚

![image-20240708220407519](./assets/image-20240708220407519.png)

### è®­ç»ƒé›†å¤§å°

| å±‚æ•° | é¢„è®­ç»ƒè¯å‘é‡ | è®­ç»ƒç­–ç•¥        | æ•°æ®é›†å¤§å° | è®­ç»ƒæ—¶é•¿  | æœ€ç»ˆlosså€¼ | æµ‹è¯•é›†BLEUå€¼      |
| ---- | ------------ | --------------- | ---------- | --------- | ---------- | ----------------- |
| ä¸¤å±‚ | æ˜¯           | Teacher Forcing | 100k       | 30.58 hr* | 0.0616     | **0.03697(3.7%)** |

é€šè¿‡å›¾è¡¨å’Œè®­ç»ƒæ›²çº¿å¯ä»¥çœ‹åˆ°åœ¨å¢åŠ æ•°æ®åŠå¤§å°ä»¥åï¼Œè™½ç„¶è®­ç»ƒæ—¶é•¿æˆå€å¢åŠ (åŸºæœ¬ä¸Šæ˜¯éšæ•°æ®é›†å¤§å°åŒæ ·å¢åŠ åå€)ï¼Œä½†æ˜¯ç”±äºä½¿ç”¨äº†**æ›´å¤§çš„æ•°æ®é›†**ï¼Œè™½ç„¶ç”±äºæ•°æ®é›†å¢å¤§å¯¼è‡´æŸå¤±å‡½æ•°æ›´éš¾ä¸‹é™ï¼ˆè™½ç„¶æˆ‘è®¤ä¸ºæ¨¡å‹çš„æ€§èƒ½çš„bottleneckå¹¶ä¸åœ¨æ˜¯å¦å……åˆ†æ‹Ÿåˆï¼‰ï¼Œæ¨¡å‹çš„**æ³›åŒ–èƒ½åŠ›å¾—åˆ°äº†å¾ˆå¥½çš„æå‡**ï¼Œè¾¾åˆ°äº†æ‰€æœ‰æ¨¡å‹æµ‹è¯•ä¸­æœ€é«˜çš„**0.03697(3.7%)**.è¿™è¯æ˜å¯¹äºNLPæœºå™¨ç¿»è¯‘ä»»åŠ¡æ¥è¯´ï¼Œé™¤äº†æ•°æ®é›†çš„è´¨é‡ä»¥å¤–ï¼Œæ•°æ®é›†çš„è§„æ¨¡ã€æ˜¯å¦æ‹¥æœ‰è¾ƒå¥½çš„æ³›åŒ–æ€§ä¸€æ ·åŠå…¶é‡è¦ã€‚

### è¿‡æ‹Ÿåˆ

å¯¹äº100kæ•°æ®é›†çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯5ä¸ªstepè®°å½•ä¸€æ¬¡æµ‹è¯•é›†çš„BLEUå€¼ï¼Œå¯ä»¥å¾—åˆ°å¦‚ä¸‹è¶‹åŠ¿ï¼š

![image-20240708222033133](./assets/image-20240708222033133.png)

å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„åœ¨æµ‹è¯•é›†ä¸Šçš„BLEUå€¼ä¸ä»…ä¸å¢é•¿ï¼Œåè€Œæ˜¯éšç€è®­ç»ƒè¿­ä»£æ¬¡æ•°å¢å¤šè€Œå‘ˆç°ä¸‹é™è¶‹åŠ¿ï¼Œè¿™ä¹Ÿå†ä¸€æ¬¡è¯æ˜äº†æœ¬æ¬¡å®éªŒä¸­ç”±äºæ•°æ®é›†è§„æ¨¡ç›¸å¯¹è¾ƒå°ï¼Œæ¨¡å‹åŸºæœ¬éƒ½å­˜åœ¨è¿‡æ‹Ÿåˆç°è±¡ã€‚å³æ¨¡å‹çš„bottleneckå¹¶ä¸åœ¨æ˜¯å¦å……åˆ†æ‹Ÿåˆï¼Œè€Œæ˜¯å—é™äºæ•°æ®é›†è§„æ¨¡ã€‚éšç€è®­ç»ƒè¿›åº¦çš„åŠ æ·±æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½åè€Œè¶Šæ¥è¶Šå·®ï¼Œåœ¨æµ‹è¯•é›†ä¸Šçš„æ•ˆæœä¹Ÿå°±è¶Šæ¥è¶Šå·®ï¼Œï¼ˆæˆ‘è‡ªå·±åœ¨è®­ç»ƒé›†ä¸Šéšæœºé‡‡æ ·æµ‹è¯•è¿‡ï¼Œè®­ç»ƒé›†çš„BLEUå¾ˆå¤šéƒ½èƒ½è¾¾åˆ°20%ä»¥ä¸Šï¼Œè€Œåœ¨æµ‹è¯•é›†ä¸­åªæœ‰4%ä¸åˆ°ï¼‰ã€‚

### attentionå¯è§†åŒ–

attentionå¯è§†åŒ–æ•ˆæœå¦‚ä¸‹ï¼š

<img src="./assets/3f500d0593d50db4faa6b19da1649d1.png" alt="3f500d0593d50db4faa6b19da1649d1" style="zoom:50%;" />

<img src="./assets/247d04c72b0b1e84a78800d194ad283.png" alt="247d04c72b0b1e84a78800d194ad283" style="zoom:50%;" />

<img src="./assets/a10aeb8896cebe5973ac2b8fd11c65d.png" alt="a10aeb8896cebe5973ac2b8fd11c65d" style="zoom:67%;" />

<img src="./assets/76a5d8a406933ed1cd754b9ff966755.png" alt="76a5d8a406933ed1cd754b9ff966755" style="zoom: 67%;" />

å¯ä»¥çœ‹åˆ°attentionå¯è§†åŒ–å¯ä»¥ä¸€å®šç¨‹åº¦ä½“ç°æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¯”å¦‚åœ¨ç¿»è¯‘â€å‘è¨€äººâ€œå’Œâ€ç§°â€œæ—¶ï¼Œæ¨¡å‹ä¸­çš„æ³¨æ„åŠ›åˆ†æ•°åœ¨"speaker"ä¸Šçš„æ³¨æ„åŠ›åˆ†æ•°æ˜æ˜¾è¾ƒé«˜ï¼Œè€Œ"speaker"çš„è¯­ä¹‰ä¹Ÿä¸â€å‘è¨€äººå’Œâ€ç§°â€æ‰€ç›¸æ¥è¿‘ã€‚è¿™ä¹Ÿåæ˜ äº†æ¨¡å‹é€šè¿‡attentionæœºåˆ¶æ•æ‰åˆ°äº†ä¸åŒè¯­ç§è¯æ±‡ä¹‹é—´çš„è¯­ä¹‰å…³è”ã€‚

## è‡ªå·±çš„æ¢ç´¢å’Œå¿ƒå¾—ä½“ä¼š

åœ¨æœ¬æ¬¡å®éªŒä¸­ï¼Œæˆ‘é€‰æ‹©äº†Seq2Seqæ¨¡å‹è¿›è¡Œä¸­è¯‘è‹±ç¿»è¯‘ï¼Œæœ€åˆé€‰æ‹©äº†åŒå±‚GRUä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨çš„ä¸»è¦ç»“æ„æ˜¯å› ä¸ºGRUåœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶è®¡ç®—æ•ˆç‡è¾ƒé«˜ï¼Œå¹¶ä¸”å¯ä»¥åœ¨è¾ƒé•¿çš„åºåˆ—ä¸Šä¿æŒè¾ƒå¥½çš„æ€§èƒ½è¡¨ç°ã€‚æ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥åˆ™æ˜¯ä¸ºäº†å…‹æœä¼ ç»ŸSeq2Seqæ¨¡å‹åœ¨é•¿åºåˆ—ç¿»è¯‘ä¸­çš„ä¸è¶³ï¼Œèƒ½å¤ŸåŠ¨æ€åœ°å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œæé«˜äº†ç¿»è¯‘çš„å‡†ç¡®æ€§å’Œæµç•…åº¦ã€‚

å…¶å®ç”±äºæ•´ä½“ä»£ç æ˜¯åœ¨pytorchå®˜æ–¹ç»™å‡ºçš„æ¡†æ¶ä¸Šå®ç°çš„ï¼Œæœ€éš¾çš„éƒ¨åˆ†åè€Œåœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼Œé¦–å…ˆå¯¹äºç¬¦å·çš„è¿‡æ»¤å’Œæ•°æ®çš„æ¸…æ´—å°±èŠ±è´¹äº†æˆ‘å¾ˆé•¿æ—¶é—´ï¼Œå› ä¸ºæ•°æ®é›†å¯èƒ½æ˜¯é€šè¿‡è§†è§‰æ–¹å¼é‡‡é›†çš„ï¼Œæœ‰å¾ˆå¤šå¥‡å¥‡æ€ªæ€ªçš„ç¬¦å·å‡ºç°ï¼Œéœ€è¦è‚‰çœ¼é€ä¸ªæ’æŸ¥è¿˜æœ‰å“ªäº›ç¬¦å·æ²¡æœ‰è¿‡æ»¤ã€‚

å…¶æ¬¡æ˜¯é¢„è®­ç»ƒè¯å‘é‡çš„åŠ è½½ï¼Œç”±äºç°æœ‰çš„é¢„è®­ç»ƒè¯å‘é‡åŸºæœ¬éƒ½æ˜¯ç™¾ä¸‡çº§åˆ«èµ·æ­¥çš„ï¼Œæˆ‘ä½¿ç”¨çš„è…¾è®¯AI labçš„ä¸¤ä¸ªé¢„è®­ç»ƒè¯å‘é‡è§£å‹ä¹‹ååŠ èµ·æ¥å¿«10ä¸ªGäº†ï¼Œå…‰æ˜¯ä¸‹è½½å°±èŠ±äº†å¾ˆé•¿æ—¶é—´ï¼Œè€Œä¸”æ¯æ¬¡è®­ç»ƒåŠ è½½éƒ½è¦åŠ è½½å¥½ä¹…ã€‚

ç„¶åæ˜¯è®­ç»ƒï¼Œè¿™æ¬¡ç®—æ˜¯æˆ‘æ•´ä¸ªå¤§å­¦ç”Ÿç”Ÿæ¶¯ä¸­è·‘è¿‡çš„æœ€å¤§çš„ç¥ç»ç½‘ç»œæ¨¡å‹äº†ï¼Œå…‰æ˜¯å°æ•°æ®é›†è·‘ä¸€æ¬¡éƒ½éœ€è¦æ¥è¿‘ä¸‰ä¸ªå°æ—¶ï¼Œè€Œå¤§æ•°æ®é›†æ›´æ˜¯è·‘äº†è¶…è¿‡30ä¸ªå°æ—¶ï¼ˆè¾›è‹¦æˆ‘çš„ç”µè„‘äº†ï¼‰ï¼Œæ•´ä¸ªå®éªŒå‘¨æœŸä¸‹æ¥ä¹Ÿæ‹‰å¾—å¾ˆé•¿ï¼Œæ˜¯æŒºè€—è´¹ç²¾åŠ›çš„ä¸€ä»¶äº‹ã€‚

æœ€åæˆ‘é€šè¿‡è¿™æ¬¡è¯•éªŒä¹Ÿäº†è§£åˆ°ä»€ä¹ˆå«â€œæ•°æ®ä¸ºç‹â€ï¼Œå“ªæ€•å®éªŒçš„æ¨¡å‹æ¶æ„å·²ç»å°½å¯èƒ½åœ°ä¼˜åŒ–ï¼Œä½†æ˜¯ç”±äºæ•°æ®é›†å¤ªå°ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¾æ—§ä¸ç†æƒ³ã€‚ä¹Ÿä½“ä¼šåˆ°äº†æ·±åº¦å­¦ä¹ çš„å·¥ä½œçœŸçš„å¾ˆè€—è´¹æ—¶é—´å’Œé‡‘é’±ï¼Œæ²¡æœ‰å¥½çš„æ˜¾å¡çš„è®¡ç®—èµ„æºæ ¹æœ¬è·‘ä¸èµ·å¤ªå¤§çš„æ¨¡å‹ã€‚

æ€»çš„æ¥è¯´è¿™æ¬¡å®éªŒè®©æˆ‘å­¦åˆ°äº†å¾ˆå¤šä¸œè¥¿ï¼Œåœ¨è¿™æ¬¡å®éªŒä¸­æˆ‘é€šè¿‡æ”¹å˜æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•ï¼Œæ¨ªå‘å¯¹æ¯”äº†å¤šç§æ–¹æ³•å’Œæ¶æ„çš„æ¨¡å‹æ•ˆæœï¼Œè®©æˆ‘å¯¹Seq2Seqæ¨¡å‹æ¶æ„çš„ç†è§£æ›´æ·±äº†ï¼Œåœ¨å®éªŒè¿‡ç¨‹ä¸­æˆ‘ä¹Ÿé‡æ–°å­¦ä¹ å’Œç†è§£äº†Teacher Forcingã€æŸæœç´¢ä»¥åŠä¸‰ç§attentionæœºåˆ¶çš„æ¦‚å¿µå’Œå®ç°æ–¹æ³•ï¼Œä»¤æˆ‘å—ç›ŠåŒªæµ…ï¼Œè®©æˆ‘åœ¨ä»¥åçš„å­¦ä¹ å’Œç ”ç©¶ä¸­æœ‰æ›´å¥½çš„åŸºç¡€ã€‚

## ä»£ç ä½¿ç”¨æ–¹æ³•

- logæ–‡ä»¶ä¸­å‚¨å­˜äº†å„ä¸ªæ¨¡å‹çš„è®­ç»ƒæŸå¤±æ›²çº¿
- saved_modelsæ–‡ä»¶ä¸­å­˜æ”¾äº†ä¿å­˜çš„checkpointsæ¨¡å‹
- è¿è¡Œ`pre_process.py`å¯ä»¥å¯¹å¯¹åº”çš„æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†
- è¿è¡Œ`train.py`å¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹ä¼šè‡ªè¡Œä¿å­˜check_pointsä»¥åŠæ—¥å¿—ã€‚
- è¿è¡Œ`evaluate.py`å¯ä»¥å¯¹æ•°æ®é›†è¿›è¡Œé¢„æµ‹ã€‚
- ç”±äºé¢„è®­ç»ƒè¯å‘é‡æ•°æ®é›†è¿‡å¤§ï¼Œsrcä¸­æ²¡æœ‰ä¿å­˜é¢„è®­ç»ƒçš„word2vecæ•°æ®é›†ï¼Œå¯èƒ½éœ€è¦è‡ªè¡Œä¸‹è½½ã€‚

