{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a11d69bce52ec88",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_top_k_zh_embeddings(glove_file_path, k):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= k:  # 只读取前k个词\n",
    "                break\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "eng_embeddings = load_top_k_zh_embeddings('data/word2vec/tencent-ailab-embedding-en-d200-v0.1.0-s.txt', k=500000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb8534a0722e7457"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 查看前五个词向量\n",
    "for i, (word, vector) in enumerate(eng_embeddings.items()):\n",
    "    print(f'{word}: ')\n",
    "    if i == 10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8441397c1480f42",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_top_k_zh_embeddings(tencent_file_path, k=500000):\n",
    "    embeddings_index = {}\n",
    "    with open(tencent_file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= k:  # 只读取前k个词\n",
    "                break\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "zh_embeddings = load_top_k_zh_embeddings('data/word2vec/tencent-ailab-embedding-zh-d200-v0.2.0-s.txt', k=500000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f9bcaabc84753a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 查看前五个词向量\n",
    "for i, (word, vector) in enumerate(zh_embeddings.items()):\n",
    "    print(f'{word}')\n",
    "    if i == 10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "607676566a974371",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " \"\"\"lang类，构建从单词到索引和从索引到单词的映射\"\"\"\n",
    "# 定义起始和结束标记的索引\n",
    "SOS_token = 0  # Start of Sentence\n",
    "EOS_token = 1  # End of Sentence\n",
    "UNK_token = 2  # Unknown word\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # 语言的名称\n",
    "        self.word2index = {}  # 单词到索引的映射\n",
    "        self.word2count = {}  # 单词出现次数的统计\n",
    "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<UNK>\"}  # 索引到单词的映射，初始包含SOS和EOS以及UNK\n",
    "        self.n_words = 3  # 单词数目初始化为3\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        # 将句子拆分成单词，并逐个添加到字典中\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        # 如果单词不在字典中，添加该单词\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words  # 给单词分配一个新的索引\n",
    "            self.word2count[word] = 1  # 初始化单词的计数为1\n",
    "            self.index2word[self.n_words] = word  # 将新索引和单词添加到索引到单词的映射中\n",
    "            self.n_words += 1  # 增加单词数目\n",
    "        else:\n",
    "            self.word2count[word] += 1  # 如果单词已存在，增加该单词的计数\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16c3cd8650ab02a6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"读取指定语言对的文本文件，创建相应的语言对象，并返回这些对象和句子对\"\"\"\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False, text_type='train'):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "\n",
    "    # 读取文件并按行分割\n",
    "    lines = open('data/sentence_pairs/%s-%s-%s.txt' % (lang1, lang2, text_type), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # 将每一行分割成对并进行规范化\n",
    "    pairs = [[s for s in l.split('@@')] for l in lines] #将每一行按照制表符 (\\t) 分割成句子对,并且每个句子对都通过标准化处理\n",
    "\n",
    "    # 如果需要反转语言对，则反转每对句子，并交换语言对象\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "699379c40a7b084",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"过滤句子对，以确保它们符合一定的条件。这些条件包括句子长度和句子前缀。\"\"\"\n",
    "\n",
    "# 定义最大句子长度\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "def filterPair(p):\n",
    "    # 检查句子对是否符合长度限制\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    # 对所有句子对进行过滤，只保留符合条件的句子对\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "533d441d4e949cc3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"定义了prepareData函数，函数通过读取文件、过滤句子对并统计词汇，最终返回处理后的语言对象和句子对。\"\"\"\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False, text_type='train'):\n",
    "    # 读取语言对文件并创建语言对象\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, text_type)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    # 过滤句子对，只保留符合条件的句子对\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    # 统计词汇\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    \n",
    "    # 输出词汇统计结果\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    # 返回处理后的语言对象和句子对\n",
    "    return input_lang, output_lang, pairs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6387789df99156de",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 准备英语到中文的数据，语言对顺序反转\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'zh', True, '100k')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd7b1024cd0430a7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 遍历output_lang的所有token，在eng_embeddings中查找对应的词向量\n",
    "i=0\n",
    "for word in output_lang.word2index.keys():\n",
    "    if word in eng_embeddings:\n",
    "        i+=1\n",
    "        print(word)\n",
    "    if i==10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29e4731875f9a5e2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 根据eng_embeddings创建词向量字典，在output_lang的词汇表中查找对应的词向量，不在的用随机初始化的词向量代替\n",
    "output_embeddings_dim = eng_embeddings['1'].shape[0]\n",
    "output_lang_embeddings = {}\n",
    "# 为SOS和EOS添加词向量\n",
    "output_lang_embeddings['<SOS>'] = np.random.uniform(-np.sqrt(1/output_embeddings_dim), np.sqrt(1/output_embeddings_dim), output_embeddings_dim)\n",
    "output_lang_embeddings['<EOS>'] = np.random.uniform(-np.sqrt(1/output_embeddings_dim), np.sqrt(1/output_embeddings_dim), output_embeddings_dim)\n",
    "output_lang_embeddings['<UNK>'] = np.random.uniform(-np.sqrt(1/output_embeddings_dim), np.sqrt(1/output_embeddings_dim), output_embeddings_dim)\n",
    "# 遍历output_lang的所有token，在eng_embeddings中查找对应的词向量\n",
    "for word in output_lang.word2index.keys():\n",
    "    if word in eng_embeddings:\n",
    "        output_lang_embeddings[word] = eng_embeddings[word]\n",
    "    else:\n",
    "        # 在-sqrt(1/dim)和sqrt(1/dim)之间均匀采样\n",
    "        output_lang_embeddings[word] = np.random.uniform(-np.sqrt(1/output_embeddings_dim), np.sqrt(1/output_embeddings_dim), output_embeddings_dim)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9ff9b701895c31d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 保证所有词都有对应的词向量\n",
    "for word in output_lang.word2index.keys():\n",
    "    assert word in output_lang_embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7912e7e93acbf1aa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output_lang_embeddings.__len__()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fec1ea95287371cd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 遍历input_lang的所有token，在zh_embeddings中查找对应的词向量\n",
    "i=0\n",
    "for word in input_lang.word2index.keys():\n",
    "    if word in zh_embeddings:\n",
    "        i+=1\n",
    "        print(word)\n",
    "    if i==10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e27b153cbd1e5a6c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 根据zh_embeddings创建词向量字典，在input_lang的词汇表中查找对应的词向量，不在的用随机初始化的词向量代替\n",
    "input_embeddings_dim = zh_embeddings['1'].shape[0]\n",
    "input_lang_embeddings = {}\n",
    "# 为SOS和EOS添加词向量\n",
    "input_lang_embeddings['<SOS>'] = np.random.uniform(-np.sqrt(1/input_embeddings_dim), np.sqrt(1/input_embeddings_dim), input_embeddings_dim)\n",
    "input_lang_embeddings['<EOS>'] = np.random.uniform(-np.sqrt(1/input_embeddings_dim), np.sqrt(1/input_embeddings_dim), input_embeddings_dim)\n",
    "input_lang_embeddings['<UNK>'] = np.random.uniform(-np.sqrt(1/input_embeddings_dim), np.sqrt(1/input_embeddings_dim), input_embeddings_dim)\n",
    "# 遍历input_lang的所有token，在zh_embeddings中查找对应的词向量\n",
    "for word in input_lang.word2index.keys():\n",
    "    if word in zh_embeddings:\n",
    "        input_lang_embeddings[word] = zh_embeddings[word]\n",
    "    else:\n",
    "        # 在-sqrt(1/dim)和sqrt(1/dim)之间均匀采样\n",
    "        input_lang_embeddings[word] = np.random.uniform(-np.sqrt(1/input_embeddings_dim), np.sqrt(1/input_embeddings_dim), input_embeddings_dim)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c676226589bbd41",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 保证所有词都有对应的词向量\n",
    "for word in input_lang.word2index.keys():\n",
    "    assert word in input_lang_embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c1d717a07e33f50",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_lang_embeddings.__len__()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed145926bca224df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 将词向量字典转换为嵌入矩阵\n",
    "def create_embeddings_matrix(embeddings, lang):\n",
    "    embeddings_matrix = np.zeros((lang.n_words, embeddings['1'].shape[0]))\n",
    "    for word, idx in lang.word2index.items():\n",
    "        embeddings_matrix[idx] = embeddings[word]\n",
    "    return torch.FloatTensor(embeddings_matrix)\n",
    "\n",
    "input_lang_embeddings_matrix = create_embeddings_matrix(input_lang_embeddings, input_lang)\n",
    "output_lang_embeddings_matrix = create_embeddings_matrix(output_lang_embeddings, output_lang)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63ff397fab76067a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 查看嵌入矩阵的形状\n",
    "input_lang_embeddings_matrix.shape, output_lang_embeddings_matrix.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d323e061dd5357e4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 编码器，将输入序列编码为隐藏状态，供解码器进一步处理\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_matrix, num_layers=2, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 使用预训练的词向量初始化嵌入层\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        \n",
    "        # GRU层，用于处理嵌入向量序列\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_p if num_layers > 1 else 0)\n",
    "        \n",
    "        # Dropout层，防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # 将输入单词索引通过嵌入层和Dropout层\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # 将嵌入向量序列输入GRU层，得到输出和隐藏状态\n",
    "        output, hidden = self.gru(embedded)\n",
    "        \n",
    "        # 返回GRU的输出和最后的隐藏状态\n",
    "        return output, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "49f7e87dd5da7846",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"注意力机制类\"\"\"\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)  # 定义线性层Wa\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)  # 定义线性层Ua\n",
    "        self.Va = nn.Linear(hidden_size, 1)  # 定义线性层Va\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))  # 计算注意力分数\n",
    "        scores = scores.squeeze(2).unsqueeze(1)  # 调整scores的形状\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)  # 计算注意力权重\n",
    "        context = torch.bmm(weights, keys)  # 计算上下文向量\n",
    "\n",
    "        return context, weights  # 返回上下文向量和注意力权重"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "81b2c5158aaebc95",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"带注意力机制的解码器\"\"\"\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embedding_matrix, num_layers=2, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # 定义嵌入层\n",
    "        self.attention = BahdanauAttention(hidden_size)  # 定义注意力机制\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_p if num_layers > 1 else 0)  # 定义GRU层\n",
    "        self.out = nn.Linear(hidden_size, output_size)  # 定义输出线性层\n",
    "        self.dropout = nn.Dropout(dropout_p)  # 定义Dropout层\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)  # 获取批次大小\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)  # 初始化解码器的输入为SOS_token\n",
    "        decoder_hidden = encoder_hidden  # 初始化解码器的隐藏状态为编码器的隐藏状态\n",
    "        decoder_outputs = []  # 用于存储解码器的输出\n",
    "        attentions = []  # 用于存储注意力权重\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(   # 进行一步解码\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)  # 将解码输出添加到列表中\n",
    "            attentions.append(attn_weights)  # 将注意力权重添加到列表中\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # 使用自己的预测作为下一步的输入\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)  # 将所有时间步的输出连接起来\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)  # 对输出进行log softmax处理\n",
    "        attentions = torch.cat(attentions, dim=1)  # 将所有时间步的注意力权重连接起来\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions  # 返回输出、隐藏状态和注意力权重\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input))  # 将输入通过嵌入层并进行Dropout\n",
    "\n",
    "        query = hidden[-1].unsqueeze(0).permute(1, 0, 2)  # 调整隐藏状态的形状以适应注意力机制，仅使用最后一层的隐藏状态\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)  # 计算上下文向量和注意力权重\n",
    "        input_gru = torch.cat((embedded, context), dim=2)  # 将嵌入向量和上下文向量连接起来\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)  # 通过GRU层\n",
    "        output = self.out(output)  # 通过线性层\n",
    "\n",
    "        return output, hidden, attn_weights  # 返回输出、隐藏状态和注意力权重"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "290149fc527f8fe2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    # 将句子中的每个单词转换为相应的索引\n",
    "    ret = []\n",
    "    for word in sentence.split(' '):\n",
    "        if word in lang.word2index:\n",
    "            ret.append(lang.word2index[word])\n",
    "        else:\n",
    "            ret.append(UNK_token)  # 如果单词不在词汇表中，用UNK_token代替\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    # 将句子转换为索引并附加EOS标记，然后转换为张量\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1) #该函数将句子转换为索引列表，并附加结束标记 EOS_token，然后将索引列表转换为张量，并返回形状为 (1, 句子长度) 的张量。\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    # 将句子对中的两个句子分别转换为张量\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor) # 该函数将句子对转换为张量对，并返回输入张量和目标张量。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c4e2b63471f0935",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"data loader\"\"\"\n",
    "\n",
    "def get_dataloader(batch_size, text_type='train'):\n",
    "    # 准备数据并创建DataLoader\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'zh', True, text_type)\n",
    "    # 调用prepareData函数，获取输入语言对象、输出语言对象和句子对列表\n",
    "\n",
    "    n = len(pairs)\n",
    "    # 获取句子对的数量\n",
    "\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    # 初始化用于存储输入和目标句子索引的数组，形状为（句子对数量, 最大句子长度）\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):        # idx: 索引，inp: 输入句子，tgt: 目标句子\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        # 将每个句子转换为索引列表\n",
    "\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        # 在每个索引列表末尾添加结束标记EOS_token\n",
    "\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "        # 将索引列表填充到数组中\n",
    "\n",
    "    # 创建TensorDataset对象，将输入和目标数组转换为张量并存储在设备上\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    # 创建随机采样器\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "\n",
    "    # 创建DataLoader对象，用于批量加载数据\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # 返回输入语言对象、输出语言对象和DataLoader对象\n",
    "    return input_lang, output_lang, train_dataloader\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d55c0492e96423be",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"训练函数\"\"\"\n",
    "\n",
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "                decoder_optimizer, criterion):\n",
    "    total_loss = 0  # 初始化总损失\n",
    "\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data  # 获取输入和目标张量\n",
    "\n",
    "        encoder_optimizer.zero_grad()  # 清零编码器的梯度\n",
    "        decoder_optimizer.zero_grad()  # 清零解码器的梯度\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)  # 前向传播，通过编码器\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)  # 前向传播，通过解码器\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()  # 反向传播计算梯度\n",
    "\n",
    "        encoder_optimizer.step()  # 更新编码器的参数\n",
    "        decoder_optimizer.step()  # 更新解码器的参数\n",
    "\n",
    "        total_loss += loss.item()  # 累加损失\n",
    "\n",
    "    return total_loss / len(dataloader)  # 返回平均损失"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ebe6fc496bb1bfc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"计算时间\"\"\"\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f671e9024e09844f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "          print_every=1, test_every=5):\n",
    "    start = time.time()  # 记录训练开始时间\n",
    "    plot_losses = []  # 用于存储绘图用的损失值\n",
    "    print_loss_total = 0  # 每个print_every轮重置\n",
    "    plot_loss_total = 0  # 每个plot_every轮重置\n",
    "\n",
    "    # 初始化优化器\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()  # 定义损失函数\n",
    "    \n",
    "    min_loss = float('inf')  # 初始化最小损失值\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # 训练一个epoch\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss  # 累加当前epoch的损失\n",
    "        plot_loss_total += loss  # 累加当前epoch的损失\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every  # 计算平均损失\n",
    "            print_loss_total = 0  # 重置损失累计\n",
    "            # 打印当前时间、epoch数、进度百分比和平均损失\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "            writer.add_scalar('loss', loss, epoch)  # 将损失写入TensorBoard\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                torch.save(encoder, f'saved_models/{running_model}/encoder_model.pth')\n",
    "                torch.save(decoder, f'saved_models/{running_model}/decoder_model.pth')\n",
    "                print('---------------------save model---------------------')\n",
    "            \n",
    "\n",
    "        if epoch % test_every == 0:\n",
    "            # 每隔test_every轮评估一次模型\n",
    "            bleu_scores, avg_bleu_score = evaluate_bleu_sacrebleu(encoder, decoder, test_pairs, input_lang, output_lang)\n",
    "            print('BLEU Score: %.4f' % avg_bleu_score)\n",
    "            writer.add_scalar('BLEU Score', avg_bleu_score, epoch)  # 将BLEU分数写入TensorBoard"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eec53c15603fc90c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"绘制损失曲线\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f5dfedd53149ad9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        # 将输入句子转换为张量\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        # 前向传播，通过编码器\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        \n",
    "        # 前向传播，通过解码器，不使用目标张量\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        # 从解码器输出中选择概率最高的单词索引\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        # 将索引转换为单词\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')  # 如果遇到结束标记，则停止翻译\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "\n",
    "    # 返回翻译的单词列表和注意力权重\n",
    "    return decoded_words, decoder_attn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b06b4e2e101f51e5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24fd2d53662d6b88",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "running_model = 'layer2_PreVec_100k'\n",
    "writer = SummaryWriter(log_dir=f'log/{running_model}')\n",
    "test_pairs = readLangs('eng', 'zh', True, 'test')[2]\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size, '100k')\n",
    "# 嵌入矩阵\n",
    "input_lang_embeddings_matrix = create_embeddings_matrix(input_lang_embeddings, input_lang)\n",
    "output_lang_embeddings_matrix = create_embeddings_matrix(output_lang_embeddings, output_lang)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, input_embeddings_dim, input_lang_embeddings_matrix).to(device)\n",
    "decoder = AttnDecoderRNN(output_embeddings_dim, output_lang.n_words, output_lang_embeddings_matrix).to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e254c421f7fdbf89",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "encoder.embedding, decoder.embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33f063a51d500423",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " train(train_dataloader, encoder, decoder, 300, print_every=1, test_every=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66dcf7205126c3a5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # 保存模型\n",
    "# torch.save(encoder, 'saved_models/layer1_PreVec/encoder_model.pth')\n",
    "# torch.save(decoder, 'saved_models/layer1_PreVec/decoder_model.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bc58b8ef1cc45a7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "# load_model = 'layer1_PreVec'\n",
    "# encoder = torch.load('saved_models/layer1_PreVec/encoder_model.pth')\n",
    "# decoder = torch.load('saved_models/layer1_PreVec/decoder_model.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90107cdf608c8198",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def ngram_counts(sentence, n):\n",
    "    \"\"\"计算句子中n-gram的数量\"\"\"\n",
    "    return Counter([tuple(sentence[i:i+n]) for i in range(len(sentence)-n+1)])\n",
    "\n",
    "def modified_precision(reference, hypothesis, n):\n",
    "    \"\"\"计算修改后的n-gram精确度\"\"\"\n",
    "    ref_counts = ngram_counts(reference, n)\n",
    "    hyp_counts = ngram_counts(hypothesis, n)\n",
    "    overlap = {ng: min(count, hyp_counts[ng]) for ng, count in ref_counts.items()}\n",
    "    return sum(overlap.values()), max(1, sum(hyp_counts.values()))\n",
    "\n",
    "def closest_reference_length(reference_lens, hyp_len):\n",
    "    \"\"\"找到与假设长度最接近的参考长度\"\"\"\n",
    "    return min(reference_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len))\n",
    "\n",
    "def brevity_penalty(reference_lens, hyp_len):\n",
    "    \"\"\"计算简洁惩罚\"\"\"\n",
    "    closest_ref_len = closest_reference_length(reference_lens, hyp_len)\n",
    "    if hyp_len > closest_ref_len:\n",
    "        return 1\n",
    "    elif hyp_len == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.exp(1 - closest_ref_len / hyp_len)\n",
    "\n",
    "def bleu(reference, hypothesis, max_n=4, smooth_factor=1e-9):\n",
    "    \"\"\"计算BLEU分数\"\"\"\n",
    "    weights = [1.0 / max_n] * max_n  # 默认权重\n",
    "    p_ns = []\n",
    "    for i in range(1, max_n + 1):\n",
    "        match_count, total_count = modified_precision(reference, hypothesis, i)\n",
    "        precision = (match_count + smooth_factor) / (total_count + smooth_factor)\n",
    "        p_ns.append(precision)\n",
    "\n",
    "    bp = brevity_penalty([len(reference)], len(hypothesis))\n",
    "    score = bp * math.exp(sum(w * math.log(p) for w, p in zip(weights, p_ns)))\n",
    "    return score\n",
    "\n",
    "# 评估BLEU分数\n",
    "def evaluate_bleu(encoder, decoder, pairs, input_lang, output_lang):\n",
    "    bleu_scores = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        target_sentence = pair[1]\n",
    "        output_words, _ = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "        bleu_score = bleu(target_sentence.split(), output_words[:-1])\n",
    "        bleu_scores.append(bleu_score)\n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    return bleu_scores, avg_bleu_score\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "975fbd2efbf907be",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def evaluate_bleu_sacrebleu(encoder, decoder, pairs, input_lang, output_lang):\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        target_sentence = pair[1]\n",
    "        output_words, _ = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "        candidate = \" \".join(output_words[:-1])\n",
    "        references = [target_sentence]\n",
    "        score = sacrebleu.corpus_bleu([candidate], [[ref] for ref in references])\n",
    "        bleu_scores.append(score.score / 100)  # 转换成小数制\n",
    "        \n",
    "    \n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    return bleu_scores, avg_bleu_score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "591b6eb7c03e1599",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 读取语言对文件并创建语言对象\n",
    "test_pairs = readLangs('eng', 'zh', True, 'valid')[2]\n",
    "# 计算BLEU分数\n",
    "bleu_scores, avg_bleu_score = evaluate_bleu_sacrebleu(encoder, decoder, test_pairs, input_lang, output_lang)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6b7af618bac1caa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "avg_bleu_score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea7758fe2b10b01c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_sentence = '马鹿 是 一个 笨蛋'\n",
    "output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "print('input =', input_sentence)\n",
    "print('output =', ' '.join(output_words))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52c7d484b9c7a591",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    # 保存注意力图\n",
    "    plt.savefig('attention.png')\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "539cba122d6383e5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "evaluateAndShowAttention('中国 与 美国 的 贸易 战争')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "12e9a3b02bfc6467",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "58772c9c84558744",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eb18dfd03aba1783"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
