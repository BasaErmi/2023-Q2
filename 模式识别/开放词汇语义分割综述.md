# 开放词汇语义分割综述

- 摘要
- 引言
- 背景
  - 问题定义、形式化描述
  - 历史及路线图
  - 相关技术
  - 数据集和指标
- 前沿方法总结
- 







## 摘要

由于昂贵的手动标记成本，现有的数据集中标注的类别通常是小规模且预定义的，即模型只会在预定义的固定类别集合内进行图像分割的任务，无法泛化到封闭词汇表之外，这也被称为封闭词汇分割（Closed Vocabulary Segmentation）。

为了突破这个限制，在过去几年中，社区对开放词汇分割（OVS）的关注日益增加。开放词汇语义分割是语义分割的一种扩展，旨在分割和识别训练集中未见过的新类别。它不仅能识别和分割已知类别，还能处理训练过程中未明确定义的类别，依赖自然语言描述来进行分割。

在这篇综述中提供了OVS最新发展的全面回顾。并且总结了目前主流OVS方法以及模型的的最新研究，介绍了各自的研究动机、技术难点和主要贡献。本文最后还提供了几个有前景的方向并进行了讨论，以启发对未来的研究。

> Semantic segmentation is a critical task in computer vision, aiming to classify each pixel in an image into a specific category at a pixel-level resolution.Due to the high cost of manual labeling, the annotated categories in existing datasets are typically small in scale and predefined. This means that models perform image segmentation within a fixed set of predefined categories and cannot generalize beyond this closed vocabulary, a limitation known as Closed Vocabulary Segmentation(CVS).
>
> To address this limitation, the communites has been growing interest in Open Vocabulary Segmentation (OVS) over the past few years. OVS is an extension of semantic segmentation that aims to segment and identify new categories not encountered during training. It can recognize and segment both known and novel categories, using natural language descriptions to guide segmentation.
>
> This review presents a comprehensive overview of the latest advancements in OVS. It summarizes the state-of-the-art OVS methods and recent research on models, discussing their research motivations, technical challenges, and major contributions. In conclusion, this review presents and discusses several promising research directions, aiming to inspire and guide future studies in the field.



## 引言

Semantic segmentation are core high-level scene perception tasks in computer vision。过去十年中，基于CNN和Transformer的模型在分割任务上取得了稳定而巨大的进步。迄今为止这项技术在自动驾驶、医学影像分析、遥感图像处理等领域具有广泛的应用。然而， Traditional Semantic segmentation methods operate on a closed vocabulary, meaning they can only recognize and segment a fixed number of classes that were present during training. However, the real world is diverse and dynamic, and it is impractical to have labeled data for every possible object or category that a model might encounter.



> Semantic segmentation is a core problem in computer vision, with the aim of partitioning an image into coherent regions with their respective semantic class labels. Over the past decade, models based on Convolutional Neural Networks (CNNs) and Transformers\cite{long2015fully, ronneberger2015u, Cheng_2022_CVPR}, have achieved substantial and consistent advancements in segmentation tasks. To date, they have been widely applied in various fields, such as autonomous driving\cite{cakir2022semantic} and medical image analysis\cite{ronneberger2015u}. However, traditional semantic segmentation methods operate on a closed vocabulary, meaning they can only recognize and segment a fixed number of classes that were present during training. Given the diversity and dynamism of the real world, it is impractical to have labeled data for every possible object or category a model might encounter.



随着研究的深入，开放词汇语义分割（OVSS）作为一种新兴的研究方向得到了广泛关注。旨在突破传统语义分割的局限，使模型能够识别和分割未见过的类别。与传统方法不同，开放词汇语义分割不再局限于训练阶段预定义的固定类别，而是希望通过模型的泛化能力和迁移学习能力，实现对新类别的识别和分割。

开放词汇语义分割的实现依赖于深度学习和自然语言处理技术的结合。特别是利用预训练的大规模视觉-语言模型（如CLIP）成为一种有效的方法。这些模型通过在大规模图像和文本数据上进行联合训练，学会了将图像和文本映射到共同的嵌入空间，从而实现了对开放词汇的识别能力。然而，如何高效地利用这些预训练模型，并在具体任务中进行适应和优化，仍然是一个亟待解决的问题。

本综述旨在系统地总结和分析开放词汇语义分割的最新进展。首先，我们将将详细介绍开放词汇语义分割的基本概念，接着在第二节回顾OVSS任务的背景，包括传统语义分割方法以及OVSS的历史和路线图。然后我们会介绍OVSS相关的前置知识，包括基于视觉-语言模型的方法、跨领域迁移学习方法以及自监督学习方法等，并总结常用的评价指标和基准数据集。在第三节，我们将OVSS的方法按照模型架构大致分为Pixel-Level Feature Matching, Two-Stage Region-Level Image-Text Matching, Improved Single-Stage, and Open Segmentation四种，并按照层次逻辑关系递进地介绍四种方法中的代表论文，并并且highlight出研究动机和技术要点。最后，我们还会总结现有的挑战，并启发未来的研究方向。



> To overcome the constraint imposed by a closed vocabulary in scene perception tasks, \textbf{Open Vocabulary Semantic Segmentation (OVSS)} has attracted significant attention as a burgeoning research field. It aims to surpass the constraints of traditional semantic segmentation by enabling models to recognize and segment categories that were not seen during training. Unlike traditional methods, OVSS is not limited to a fixed set of predefined categories but leverages the generalization and transfer learning abilities of models to identify and segment new categories.
>
> The realization of OVSS depends on the integration of deep learning and natural language processing techniques. Specifically, the use of large-scale pre-trained vision-language models, such as CLIP\cite{radford2021learning}, has proven to be an effective strategy. These models are jointly trained on extensive image and text datasets, learning to map images and texts into a shared embedding space, thereby enabling open vocabulary recognition. However, effectively utilizing these pre-trained models and adapting and optimizing them for specific tasks remains a significant challenge.
>
> This review systematically summarizes and analyzes the latest advancements in OVS. We begin by reviewing the evolution of traditional semantic segmentation methods, highlighting their limitations and challenges. We then detail the fundamental concepts and implementation strategies of OVS, including methods based on vision-language models, cross-domain transfer learning, and self-supervised learning. Furthermore, we discuss the practical applications and performance of OVS in various scenarios, and we provide a summary of common evaluation metrics and benchmark datasets.





## 预备知识

### 形式化定义

设 \( X \) 表示输入图像，\( Y \) 表示对应的像素级别类别标签。传统的语义分割模型学习一个映射函数 \( f : X \rightarrow Y \)，其中 \( Y \) 属于一个固定的类别集 \( C_{\text{train}} \)。

在开放词汇语义分割中，目标是学习一个映射函数 \( f' : X \rightarrow Y' \)，其中 \( Y' \) 包含了训练期间未见过的类别，表示为 \( C_{\text{open}} \)。这意味着在测试时，模型应该能够分割新的类别 \( C_{\text{test}} \)，使得 \( C_{\text{test}} \cap C_{\text{train}} = \emptyset \)，但 \( f' \) 仍然能够准确地分割 \( C_{\text{test}} \)。

> Let \( X \) represent an input image and \( Y \) represent the corresponding pixel-level category labels. Traditional semantic segmentation models learn a mapping function \( f : X \rightarrow Y \), where \( Y \) belongs to a fixed category set \( C_{\text{train}} \).
>
> In open vocabulary semantic segmentation, the goal is to learn a mapping function \( f' : X \rightarrow Y' \), where \( Y' \) includes categories beyond those seen during training, represented as \( C_{\text{open}} \). This means that during testing, the model should be able to segment new categories \( C_{\text{test}} \) such that \( C_{\text{test}} \cap C_{\text{train}} = \emptyset \), yet \( f' \) should still accurately segment \( C_{\text{test}} \).

### 历史

最早利用神经网络进行语义分割的工作是由Jonathan Long等人在2015年提出的，他们的工作首次提出了将传统的卷积神经网络（CNN）转换为全卷积网络（FCN），以实现端到端的像素级语义分割。这项工作被视为现代语义分割研究的奠基之作，为后续的许多研究，如U-Net和Deeplab等奠定了基础。然而，这些传统的语义分割模型依赖于在训练阶段预定义的一组固定类别，无法对训练集之外的类别进行分类。

> The earliest work utilizing neural networks for semantic segmentation was proposed by Jonathan Long et al. in 2015. Their paper, *Fully convolutional networks for semantic segmentation*, was the first to introduce the conversion of traditional CNNs into Fully convolutional networks(FCNs) for end-to-end pixel-level semantic segmentation. This pioneering work is regarded as a cornerstone of modern semantic segmentation research, establishing the foundation for subsequent models such as U-Net and Deeplab. However, these traditional semantic segmentation models depend on a predefined set of fixed categories during the training phase and are incapable of classifying categories beyond the training set.

随着BERT [9]在自然语言处理（NLP）领域的崛起，多模态预训练受到了广泛关注。受到视觉语言预训练过程的启发，OVR-CNN [11]提出了开放词汇目标检测的概念，通过使用标题数据来连接新颖类别语义和视觉区域。后来，CLIP[12]使得模型能够在没有明确训练过的类别上进行零样本分类和识别。LSeg [14]首次探索了CLIP在语言驱动分割任务中的应用。

在这些工作的基础上，近年来越来越多的研究致力于提高开放词汇分割的性能，OpenSeg[13]提出由像素级特征匹配转换为区域级特征匹配，模型修改为二阶段结构。OVSeg [15]显著提升了二阶段区域级图文匹配的性能。SAM [42]提出了构建分割基础模型的概念，并训练了数十亿级别的掩码。结合CLIP，SAM在无需微调的情况下也能实现良好的零样本分割效果。

时至今日，随着大型语言模型（LLM）的快速发展，开放词汇学习成为计算机视觉领域一个更加有前景的研究方向。

> With the rise of BERT [9] in natural language processing (NLP), multimodal pretraining has garnered significant attention. Inspired by visual-language pretraining, OVR-CNN [11] introduced the concept of open-vocabulary object detection, utilizing caption data to connect novel category semantics with visual regions. Subsequently, CLIP [12] enabled models to perform zero-shot classification(ZSC) and recognition on categories that were not explicitly included during training. LSeg [14] was the first to investigate the application of CLIP in language-driven segmentation tasks.OpenSeg [13] proposed shifting from pixel-level feature matching to region-level feature matching, adapting the model to a two-stage structure. Building on these foundational works, OVSeg [15] significantly improved the performance of two-stage region-level image-text matching. SAM [42] introduced the concept of a segmentation foundation model, training on billions of masks. When combined with CLIP, SAM can achieve robust zero-shot segmentation results without requiring fine-tuning.
>
> With the rapid advancement of large language models (LLM) [43], open-vocabulary learning has become an even more promising direction, as more extensive language knowledge can be integrated into multimodal architectures.

### 传统闭集语义分割

FCN将语义分割问题处理为密集像素分类问题。随后，许多工作都基于FCN框架进行了改进和扩展。例如，DeepLab通过扩展卷积、条件随机场（CRF）和空洞空间金字塔池化（ASPP）增强了FCN的性能。U-Net则通过多尺度特征融合，更好地提取了各个尺度上的特征信息。 在Transformer提出之后，为了更好地进行全局上下文建模，一些研究提出了自注意力操作的变种，以替代传统卷积神经网络（CNN）的预测头。

> The modern semantic segmentation problem, first introduced by FCN in 2015, is typically approached as a dense pixel classification task. Following FCN, numerous studies have sought to improve and expand upon its framework. For instance, DeepLab enhanced FCN's performance using dilated convolutions, conditional random fields and atrous spatial pyramid pooling. U-Net, on the other hand, improved multi-scale feature extraction through multi-scale feature fusion.
>
> With the advent of Transformers in 2017, some research has proposed variants of self-attention mechanisms to better model global context, replacing the traditional CNN prediction heads. These advancements aim to exploit the strengths of Transformers, such as their ability to capture long-range dependencies, thereby further improving the effectiveness of semantic segmentation.

### 视觉-语言预训练模型

视觉-语言预训练模型是近年来在计算机视觉和自然语言处理领域中的一个重要进展。这类模型通过联合图像和文本数据进行训练，学会了将图像和文本映射到共同的嵌入空间。

- **CLIP**：由OpenAI提出的CLIP模型通过对比学习将自然语言和图像进行对齐，从而在大规模图文对数据上进行预训练。CLIP利用一个文本编码器和一个图像编码器，将文本和图像嵌入到同一个向量空间中，使得相同语义的文本和图像的嵌入向量接近。该模型能够在没有任务特定数据集的情况下，利用文本描述来理解和分类未见过的图像类别。

  > **CLIP**: Proposed by OpenAI, the CLIP model uses contrastive learning to align natural language with images, enabling pretraining on large-scale image-text datasets. CLIP employs a text encoder and an image encoder to embed both text and images into a shared vector space, ensuring that the embedding vectors of semantically similar text and images are close to each other. This model can understand and classify previously unseen image categories using textual descriptions, without requiring task-specific datasets.

- **ALIGN**：ALIGN模型由Google提出，它通过对海量的图像和其对应的文本描述进行训练，类似于CLIP的对比学习方法。ALIGN在训练过程中使用了数十亿的图像-文本对，使得模型具备了强大的泛化能力和对开放词汇的识别能力。

视觉-语言预训练模型的核心优势在于它们能够在没有明确标注的新类别上进行推理和分类。这使得它们在开放词汇语义分割任务中具有重要应用，因为它们可以利用自然语言描述来识别和分割新的图像类别，而不需要重新训练模型。

### 迁移学习和自监督学习

迁移学习和自监督学习是提升模型泛化能力的重要技术，使模型能够适应新的类别和数据分布。

- **迁移学习**：迁移学习指的是将一个在大规模数据集上预训练好的模型应用到新的任务中，通常只需要对模型进行微调（fine-tuning）。在语义分割任务中，可以将预训练好的视觉-语言模型（如CLIP、ALIGN）的权重作为初始权重，然后在目标数据集上进行微调。这不仅加速了模型的训练过程，还能显著提高模型在新任务上的表现。

- **自监督学习**：自监督学习通过设计预任务（pretext task），使模型在没有人工标注数据的情况下进行训练。例如，DINO（Self-Distillation with No Labels）和MAE（Masked Autoencoders）是两种常见的自监督学习方法。DINO通过一种自我蒸馏的方式，使模型通过对不同视角的图像进行一致性学习，从而获取有效的特征表示。MAE通过遮掩图像的一部分并训练模型重建这些部分，从而学习到图像的全局和局部特征。这些自监督学习方法能够在大规模无标注数据上训练，从而提升模型的泛化能力。

### 参数高效微调

**参数高效微调 (Parameter-Efficient Fine-Tuning)** 是一种通过对预训练模型的少量参数进行调整，达到在特定任务上高效微调的方法。这种方法的优势在于，只需调整一小部分参数即可适应新任务，而无需重新训练整个模型，从而节省计算资源和时间。在开放词汇语义分割中，PEFT方法可以通过微调预训练的视觉语言模型，使其在特定的语义分割任务上表现更好。

### 数据集和评估指标

\\\textbf{Metrics.}The commonly used metrics are mean intersection over union (mIoU), 用于衡量预测结果与真实标注之间的重叠程度。mIoU is calculated as the following:
\begin{equation}
  \text{mIoU} = \frac{1}{N} \sum_{i=1}^{N} \frac{TP_i}{TP_i + FP_i + FN_i}
  \label{eq:miou}
\end{equation}

 其中
 \( TP_i \)（True Positives）是预测为类别 \( i \) 且实际为类别 \( i \) 的像素数量。
\( FP_i \)（False Positives）是预测为类别 \( i \) 但实际不是类别 \( i \) 的像素数量。
\( FN_i \)（False Negatives）是实际为类别 \( i \) 但预测为其他类别的像素数量。



## 方法概述

在这一小节，我们将会按照模型架构将模型分为Pixel-Level Feature Matching，Two-Stage Region-Level Image-Text Matching，Improved Single-Stage  以及Open Segmentation 四类。并且会按照技术路线和逻辑层次关系递进地介绍OVSS近三年来的主流模型，并且highlight出研究动机和技术要点。

#### LSeg

Language-driven Semantic Segmentation（LSeg）首次探索了CLIP在语言驱动分割任务中的应用，是第一个将开放词汇概念引入语义分割领域的工作。LSeg将传统封闭词汇分类器替换为基于图文特征比对的开放分类器（例如CLIP），并且提出像素级特征匹配，实现了像素-文本特征对齐。具体来说，LSeg使用文本编码器计算描述性输入标签的嵌入，并使用基于Transformer的图像编码器计算输入图像的密集逐像素嵌入。图像编码器通过对比目标进行训练，以使像素嵌入与对应语义类别的文本嵌入对齐。文本嵌入提供了一种灵活的标签表示，其中语义相似的标签在嵌入空间中映射到相似的区域，这使得LSeg在测试时能够泛化到之前未见过的类别，无需重新训练或额外的训练样本。

LSeg的算法流程图见图1.详细来说，首先使用冻结参数的CLIP文本编码器提取类别文本特征，输入的类别标签集通过CLIP文本编码器生成N个文本嵌入向量 \( T \in \mathbb{R}^{N \times C} \)。其次输入的图像通过Dense Prediction Transformers (DPT)图像编码器生成逐像素的图像嵌入 \( I \in \mathbb{R}^{\tilde{H} \times \tilde{W} \times C} \)。

Next，将CLIP文本特征和图像特征图逐像素计算点积相似性，得到逐像素的相关张量 \( F = I \cdot T \in \mathbb{R}^{\tilde{H} \times \tilde{W} \times N} \)。

最后，通过空间正则化模块对预测结果进行空间正则化和上采样输出分割结果图，其中每个像素被分配到最相似的标签类别，并逐像素计算交叉熵损失进行训练。

> Language-driven Semantic Segmentation (LSeg) \cite{lseg} was the first to explore the application of CLIP in language-driven segmentation tasks and to introduce the concept of open vocabulary into semantic segmentation. LSeg replaces the traditional closed vocabulary classifier with an open classifier based on image-text feature matching, such as CLIP\cite{climp}, and proposes pixel-level feature matching to achieve pixel-text feature alignment. Specifically, LSeg uses a text encoder to compute embeddings of descriptive labels and a Transformer-based image encoder to compute dense per-pixel embeddings of input images. The image encoder is trained with contrastive learning objectives to align pixel embeddings with the corresponding semantic category text embeddings. Text embeddings provide a flexible label representation, where semantically similar labels are mapped to similar regions in the embedding space. This allows LSeg to generalize to previously unseen categories during testing without requiring retraining or additional training samples.
>
> The algorithm flow of LSeg is shown in ~\figurename~\ref{fig:lseg}. Specifically, the process begins with using a frozen-parameter CLIP text encoder to extract category text features. The input category label set generates \( N \) text embedding vectors \( T \in \mathbb{R}^{N \times C} \) through the CLIP text encoder. Next, the input image is processed by the Dense Prediction Transformers (DPT) image encoder to produce per-pixel image embeddings \( I \in \mathbb{R}^{\tilde{H} \times \tilde{W} \times C} \).The CLIP text features and the image feature map are then used to compute the dot product similarity per pixel, resulting in a per-pixel correlation tensor \( F = I \cdot T \in \mathbb{R}^{\tilde{H} \times \tilde{W} \times N} \).Finally, the prediction results undergo spatial regularization and upsampling through a spatial regularization module, producing the segmentation result map. Each pixel is assigned to the most similar label category, and cross-entropy loss is calculated per pixel for training.
>
> The experimental results demonstrate that, under zero-shot settings, LSeg achieves significant improvements compared to existing methods. However, its segmentation performance still lags behind traditional few-shot methods. Overall, as a pioneering work in the field of OVSS, LSeg has inspired a new paradigm of pixel-level image-text matching to achieve open-vocabulary segmentation.



### OpenSeg

虽然LSeg模型将开放词汇的概念引入了语义分割，但是这类直接进行像素级特征匹配的模型最大的问题在于训练数据的可拓展性。像素级的匹配需要进行像素级的标注，而一般情况下获取像素级开放词汇标注是非常困难和昂贵的。除此之外，直接进行像素匹配也会导致混乱且边界不清晰的问题。基于此，Golnaz Ghiasi等人提出了OpenSeg，通过先提取出掩码区域再掩码区域进行区域级匹配，将特征匹配从像素级转换到了区域级，由此模型修改为了二阶段结构。区域级特征匹配与像素级特征匹配的区别可见图2.

> Although the LSeg introduced the concept of open vocabulary into semantic segmentation, the main issue with models that perform direct pixel-level feature matching is the scalability of training data. Pixel-level matching requires pixel-level annotations, which are typically very difficult and expensive to obtain for open vocabulary labels. Moreover, direct pixel matching can result in confusion and unclear boundaries. To address these challenges, Golnaz Ghiasi et al. proposed OpenSeg. This model first extracts mask regions and then performs region-level matching within these regions, shifting feature matching from the pixel level to the region level and thus transforming the model into a two-stage structure. The difference between region-level feature matching and pixel-level feature matching is illustrated in Figure 2.

OpenSeg提出了一种使用提案掩码及其特征 \( Z \in \mathbb{R}^{N \times D} \) 表示图像的新方法，能够通过弱监督学习从图像标题中精确地学习图像分割。首先，使用特征金字塔网络（FPN）和交叉注意力模块从图像中提取多尺度特征 \( F \)，并通过卷积和全连接层获得增强图像特征 \( F_{PEs} \)。通过计算掩码查询 \( q \) 和位置增强图像特征的点积来预测掩码 \( s = \text{Sigmoid}(\text{dot}(q, F_{PEs})) \)。优化过程中，通过计算预测掩码 \( s \) 和无类别标注掩码 \( s_l \) 之间的Dice系数并最大化其相似性来优化掩码匹配：
\[ 
L_S = \frac{1}{M} \sum_{j=1}^M \left(1 - \max_i \text{Dice}(s_i, s_{lj})\right) 
\]

为了实现视觉-语义对齐，OpenSeg将图像区域与标题中的词进行对齐，通过计算区域特征 \( z \) 和词特征 \( w \) 的余弦相似性来评估相似度，并最大化标注图像-标题对的归一化分数。区域 \( i \) 和词 \( j \) 的相似性分数由余弦相似性定义 \( \langle z_i, w_j \rangle = \frac{z_i \cdot w_j}{\|z_i\|\|w_j\|} \)。图像 \( I_b \) 和标题 \( C_b \) 的相似性计算公式为：
\[ 
G(I_b, C_b) = \frac{1}{K} \sum_{j=1}^K \sum_{i=1}^N \sigma(g(z, w_j))_i \cdot \langle z_i, w_j \rangle 
\]
其中 \( \sigma \) 表示Softmax函数。对齐损失旨在最大化标注图像-标题对在所有图像和标题中的归一化分数：
\[ 
L_G = - \frac{1}{|B|} \sum_{b=1}^{|B|} \left(\log \sigma(G(I, C_b))_b + \log \sigma(G(I_b, C))_b\right) 
\]

为了扩展训练数据，OpenSeg采用一种自训练方法，首先在分割数据集上训练教师模型，仅使用分割损失 \( L_S \)。然后使用教师模型为大规模图像-文本数据集生成伪分割标签，最终在混合了人工和伪标签的数据上训练模型。最终的总损失为：
\[ 
L = L_G + \alpha L_S 
\]
这种方法允许模型在开放词汇分割任务中处理任何数量的类别，并提供高质量的分割预测结果。OpenSeg在多个数据集上都展现出了优越的性能，尤其是在零样本和少样本设置下。而且在未见类别上也具有很强的泛化能力。

## ZegFormer

与OpenSeg的工作类似，Jian Ding等人同样对像素级别的特征匹配进行了改进。针对零样本语义分割(ZS3)任务提出了ZegFormer，将分割问题解耦为二阶段问题，分为类无关掩码生成和掩码分类两个阶段。同时额外引入视觉-语言模型来提升掩码分类准确度，通过掩码从原图得到掩码图后再利用视觉-语言模型对掩码图进行分类。

ZegFormer首先生成一组分段级嵌入，并通过两个平行层进行类别无关分组和分段级零样本分类。选择Maskformer作为基础语义分割模型，通过将 \(N\) 个分段查询和一个特征图输入Transformer解码器，生成分段嵌入 \(G_q \in \mathbb{R}^d\) 和掩码嵌入 \(B_q \in \mathbb{R}^d\)。

在类别无关分组阶段，ZegFormer使用二进制掩码预测将像素解码器输出的特征图 \(F(I) \in \mathbb{R}^{d \times H \times W}\) 进行分组，掩码预测 \(m_q = \sigma(B_q \cdot F(I)) \in [0, 1]^{H \times W}\)。利用SSE进行分段分类时，将类别名称放入提示模板中并输入文本编码器以获得文本嵌入 \(T = \{T_c \in \mathbb{R}^d | c = 1, \ldots, |C|\}\)，通过余弦相似度计算分段预测概率分布：
\[ p_q(c) = \frac{\exp(\frac{1}{\tau} \text{sc}(T_i, G_q))}{\sum_{i=0}^{|C|} \exp(\frac{1}{\tau} \text{sc}(T_i, G_q))} \]
其中，\(\text{sc}(e, e') = \frac{e \cdot e'}{|e||e'|}\)，\(\tau\) 为温度参数。

训练过程中，使用Dice损失和Focal损失的组合来计算掩码损失 \(L_{\text{mask}}(m_q, R_{gt_q})\)。在推理过程中整合预测的二进制掩码和分段分类得分以获得最终结果，并提出了ZegFormer的三个变体：

1. **ZegFormer-seg**：使用分段查询的分段分类得分，通过计算每个像素的类别概率：
  \[ \sum_{q=1}^N p_q(c) \cdot m_q[h, w] \]
  校准预测，通过减少已见类别的得分，每个像素的最终类别预测为：
  \[ \arg \max_{c \in S+U} \left(\sum_{q=1}^N p_q(c) \cdot m_q[h, w] - \gamma \cdot I[c \in S]\right) \]
  其中 \(\gamma \in [0, 1]\) 为校准因子，指示函数 \(I\) 在 \(c\) 属于已见类别时为1。

2. **ZegFormer-img**：推理过程与公式（2）相似，唯一的区别是 \(p_q(c)\) 被 \(p'_q(c)\) 替代。

3. **ZegFormer**：融合 \(p_q(c)\) 和 \(p'_q(c)\)：
  \[ p_{q, \text{fusion}}(c) = \begin{cases} 
  p_q(c)^{1-\lambda} \cdot p_{q, \text{avg}}^\lambda & \text{如果 } c \in S \\
  p_q(c)^{1-\lambda} \cdot p'_q(c)^\lambda & \text{如果 } c \in U
  \end{cases} \]
  其中 \(\lambda\) 用于平衡两个分类得分的贡献。当 \(c\) 属于 \(S\) 时，计算 \(p_q(c)\) 和 \(p_q, \text{avg}} = \sum_{j \in S} p'_q(j)/|S|\) 的几何平均。最终的语义分割结果通过类似公式（2）的过程获得。

ZegFormer通过融合不同分类得分的贡献，调整已见类别和未见类别的概率范围，可以实现更精确的语义分割。



### OVSeg

ZegFormer等基于二阶段区域级图文匹配的模型出现以后，仍有许多研究人员尝试改进这类模型的性能。Feng经过实验发现，由于CLIP 的训练场景与掩码图像存在较大分布差异， "Candidate Region Generation + Ground Truth Classification" 的组合 significantly outperforms "Ground Truth Region + Predicted Classification"组合. 这意味着二阶段开放词汇语义分割方法的性能瓶颈在于区域掩码分类。

ZegFormer通过微调CLIP的方式提升模型性能。首先需要基于全局图文对数据构建合适的 CLIP 区域微调数据：如图3所示，利用已有监督数据训练MaskFormer，然后提取候选掩码，为图像生成掩码图像。接着从caption中提取类别名称，最后使用 CLIP 将掩码图像与类别名称计算相 似性，匹配最高的类别赋予伪标签。

构建数据完成后，使用Mask Prompt Tuning的方法来完成对CLIP的微调，使其适应掩码图像输入。如图4所示，首先利用已有的标注数据训练MaskFormer模型，然后提取候选掩码以生成掩码图像。接着从图像的描述文本（caption）中提取类别名称，并使用CLIP计算掩码图像与类别名称的相似性，为相似度最高的类别赋予伪标签。为了使CLIP适应掩码图像输入，在模型中添加可学习的mask prompt token，用来替换不包含掩码区域的patch embedding。在微调过程中，损失函数与CLIP保持一致，但只更新mask prompt token，并冻结CLIP的原始参数。通过这个过程生成带有伪标签的掩码图像数据，用于微调CLIP模型，以提升其在掩码图像上的分类性能。

> ### Direct Translation
> After the emergence of models like ZegFormer, which are based on two-stage region-level image-text matching, many researchers have continued to try to improve the performance of such models. Feng et al. found through experiments that, due to the significant distribution differences between CLIP's training scenarios and mask images, the combination of "Candidate Region Generation + Ground Truth Classification" significantly outperforms the combination of "Ground Truth Region + Predicted Classification". This means that the performance bottleneck of two-stage open-vocabulary semantic segmentation methods lies in region mask classification.
>
> ZegFormer improves model performance by fine-tuning CLIP. First, it is necessary to construct suitable CLIP region fine-tuning data based on global image-text pairs: as shown in Figure 3, MaskFormer is trained using existing supervised data, then candidate masks are extracted to generate masked images for the image. Next, category names are extracted from captions, and finally, CLIP is used to calculate the similarity between the masked images and the category names, assigning pseudo-labels to the highest-matching categories.
>
> Once the data is constructed, Mask Prompt Tuning is used to fine-tune CLIP to adapt to masked image input. As shown in Figure 4, MaskFormer is first trained using existing annotated data, then candidate masks are extracted to generate masked images. Then, category names are extracted from the image's descriptive text (caption), and CLIP is used to calculate the similarity between the masked images and the category names, assigning pseudo-labels to the highest-matching categories. To adapt CLIP to masked image input, a learnable mask prompt token is added to the model, replacing the patch embedding that does not contain masked regions. During fine-tuning, the loss function remains consistent with CLIP, but only the mask prompt token is updated, and CLIP's original parameters are frozen. Through this process, masked image data with pseudo-labels is generated, which is used to fine-tune the CLIP model and improve its classification performance on masked images.
>
> ***
>
> ### Identified Issues
> 1. **"After the emergence of models like ZegFormer, which are based on two-stage region-level image-text matching"**: Clear, but could be more concise.
> 2. **"Many researchers have continued to try to improve the performance of such models"**: Clear and correct.
> 3. **"Feng et al. found through experiments that, due to the significant distribution differences between CLIP's training scenarios and mask images"**: Clear, but could be slightly rephrased for better flow.
> 4. **"The combination of 'Candidate Region Generation + Ground Truth Classification' significantly outperforms the combination of 'Ground Truth Region + Predicted Classification'"**: Clear and correct.
> 5. **"This means that the performance bottleneck of two-stage open-vocabulary semantic segmentation methods lies in region mask classification"**: Clear and correct.
> 6. **"ZegFormer improves model performance by fine-tuning CLIP"**: Clear, but could be more specific.
> 7. **"Construct suitable CLIP region fine-tuning data based on global image-text pairs"**: Clear and correct.
> 8. **"MaskFormer is trained using existing supervised data, then candidate masks are extracted to generate masked images for the image"**: Clear, but a bit lengthy; could be split.
> 9. **"Category names are extracted from captions, and finally, CLIP is used to calculate the similarity between the masked images and the category names, assigning pseudo-labels to the highest-matching categories"**: Clear and correct.
> 10. **"Once the data is constructed, Mask Prompt Tuning is used to fine-tune CLIP to adapt to masked image input"**: Clear, but could be more concise.
> 11. **"MaskFormer is first trained using existing annotated data, then candidate masks are extracted to generate masked images"**: Clear and correct.
> 12. **"Category names are extracted from the image's descriptive text (caption), and CLIP is used to calculate the similarity between the masked images and the category names, assigning pseudo-labels to the highest-matching categories"**: Clear, but could be more concise.
> 13. **"To adapt CLIP to masked image input, a learnable mask prompt token is added to the model, replacing the patch embedding that does not contain masked regions"**: Clear and correct.
> 14. **"During fine-tuning, the loss function remains consistent with CLIP, but only the mask prompt token is updated, and CLIP's original parameters are frozen"**: Clear and correct.
> 15. **"Through this process, masked image data with pseudo-labels is generated, which is used to fine-tune the CLIP model and improve its classification performance on masked images"**: Clear and correct.
>
> ***
>
> ### Reinterpreted Translation
> After the introduction of models like ZegFormer, which are based on two-stage region-level image-text matching, many researchers have sought to further improve the performance of these models. Feng et al. discovered that, due to the significant distribution differences between CLIP's training data and masked images, the combination of "Candidate Region Generation and Ground Truth Classification" significantly outperforms "Ground Truth Region and Predicted Classification." This indicates that the performance bottleneck of two-stage open-vocabulary semantic segmentation methods lies in region mask classification.
>
> ZegFormer enhances model performance by fine-tuning CLIP. First, suitable CLIP region fine-tuning data based on global image-text pairs is constructed: as shown in Figure 3, MaskFormer is trained using existing supervised data, and candidate masks are extracted to generate masked images. Then, category names are extracted from captions, and CLIP calculates the similarity between the masked images and the category names, assigning pseudo-labels to the best-matching categories.
>
> After constructing the data, Mask Prompt Tuning is used to fine-tune CLIP for masked image inputs. As shown in Figure 4, MaskFormer is first trained with existing annotated data, and candidate masks are extracted to generate masked images. Then, category names are extracted from the captions, and CLIP calculates the similarity between the masked images and the category names, assigning pseudo-labels to the most similar categories. To adapt CLIP to masked image input, a learnable mask prompt token is added, replacing patch embeddings that do not contain masked regions. During fine-tuning, the loss function remains consistent with CLIP, but only the mask prompt token is updated, with CLIP's original parameters frozen. This process generates masked image data with pseudo-labels, which is used to fine-tune the CLIP model and improve its classification performance on masked images.

### SAN

由于segmentation 数据集的数据量远小于视觉语言预训练数据集，因此微调模型在开放词汇识别方面的能力往往受到限制。也就是说对图文预训练模型CLIP进行微调会影响模型的泛化性（例如LSeg等），减弱模型对于新类的预测。同时，生成掩码图像再利用 CLIP 分类的二阶段方法需要使用 CLIP 多次推理，而且掩码预测模型完全独立于视觉语言预训练模型，导致生成的掩码图像裁剪可能不适合识别，最终导致模型笨重、缓慢且性能较差。基于此，Xu等人提出了一种无需微调，一次推理的高效开放词汇语义分割框架，名为Side Adapter Network（SAN）。

SAN基于CLIP引入了side adapter network， 生成掩码及掩码注意力，引导CLIP更关注掩码区域，提高分类性能。SAN 由一个轻量级vision-transformer 实现，输入图像被分割为 16×16 的块，经过线性嵌入层投影为视觉 tokens，并与可学习查询 tokens 连接后输入 transformer 层。SAN 生成掩码提议和相应的注意力偏置，这些偏置用于掩码识别。掩码通过查询 tokens 和视觉 tokens 的内积生成，公式如下：
\[ M = V_{mask}Q_{mask}^T \]
注意力偏置通过类似的方法生成并应用于 CLIP 的自注意力层，公式如下：
\[ B = V_{attn}Q_{attn}^T \]
掩码预测和识别的解耦设计使得用于识别掩码的感兴趣区域可以与掩码区域本身不同，从而提高性能。

为了进一步利用 CLIP 的强大特征，SAN还进行特征融合，将 CLIP 的视觉 tokens 融合到 SAN 中。通过逐层融合 CLIP 和 SAN 的特征，显著提高了模型性能。此外，SAN还引入了影子 [CLS] tokens（[SLS] tokens），用于在不改变 CLIP 模型参数的情况下，通过注意力偏置引导 [CLS] token 的注意力图，从而实现更精确的掩码识别。

在训练过程中，SAN通过 dice 损失 \( L_{mask\_dice} \) 和二元交叉熵损失 \( L_{mask\_bce} \) 来监督掩码生成，通过交叉熵损失 \( L_{cls} \) 来监督掩码识别。总损失包括这些损失的加权和，公式如下：
\[ L_{seg} = \lambda_1L_{mask\_dice} + \lambda_2L_{mask\_bce} + \lambda_3L_{cls} \]
 其中λ1, λ2, λ3为损失权重 。通过端到端训练，侧适配器网络可以最大限度地适应冻结的 CLIP 模型，使得掩码提议和注意力偏置是 CLIP 感知的。

实验结果表明，深层特征比浅层特征更具语义性，多层特征融合比单层融合更能提高性能。此外，SAN采用了单次前向设计，浅层 CLIP 用于特征融合，深层 CLIP 用于掩码识别，从而减少了计算成本，并且在多个基准测试中显著优于现有方法。



### FC-CLIP

不仅仅只有SAN尝试对二阶段模型进行改进，Yu等人开发了另一种单阶段的开放词汇语义分割算法框架FC-CLIP。FC-CLIP主要通过共享一个冻结的CLIP卷积主干，实现了掩码生成和分类的统一。在双阶段方法中，首先使用掩码生成器生成图像的候选掩码区域，然后将这些区域输入CLIP模型进行分类，CLIP模型分别执行掩码特征提取和文本特征提取，并进行匹配。然而，这种方法需要多次特征提取，效率较低。而在FC-CLIP中，通过使用一个共享的冻结CLIP卷积主干，图像特征可以同时用于掩码生成和分类，从而避免了重复的特征提取。CLIP卷积主干不仅保留了预训练的图像-文本对齐特性，还能生成高质量的掩码和进行准确的分类，从而显著提高了效率和性能。

由于FC-CLIP使用一个共享的、冻结的卷积CLIP主干进行掩码生成和分类相较于基于ViT的CLIP模型（比如SAN），基于卷积的CLIP模型在在密集预测所需的高分辨率图像上泛化性更好 ，而且特征更加平滑、更适合 mask-pooling 提取掩码区域特征。与此同时，FC-CLIP在保持训练和测试中成本非常低的同时精度也显著优于同期其它方法



## SAM

The Segment Anything Model (SAM) developed by Meta AI is a cutting-edge AI model designed for promptable segmentation. It excels at segmenting objects within images with a single click, demonstrating zero-shot generalization to unfamiliar objects and images without requiring additional training. This capability is enabled by its architecture, which includes an image encoder, a prompt encoder, and a mask decoder working together to deliver precise segmentation outputs.

SAM leverages advanced technologies such as Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs) to perform detailed image analysis and generate accurate segmentation masks. CNNs help in recognizing and interpreting patterns in images, while GANs enhance the model's ability to create lifelike and precise segmentations. This combination allows SAM to handle a wide array of visual inputs with high precision, making it a significant advancement in the field of image segmentation .

A key factor in SAM's effectiveness is its extensive training on the SA-1B dataset, which includes over 1 billion segmentation masks from 11 million images. This vast and diverse dataset ensures that SAM can generalize well across various tasks and environments, enhancing its applicability in numerous domains such as AI-assisted labeling, medical imaging, and land cover mapping.

